{"cells":[{"cell_type":"markdown","source":["# Allstate Insurance Claims \n#### W261, Final Project Spring 2020, Team 10\n\nAuthors:\n- Yang Jing\n- Ryan Keunho Kim\n- Christine Barger \n- Sophia Cui\n\nCode:\n- [Github Repository](https://github.com/UCB-w261/project-sp20-team-10)\n    - Supplementary notebooks are in corresponding folders in the repository\n- [Public Notebook with Results](https://ucb-w261.github.io/project-sp20-team-10/W261_SP20_FINAL_PROJECT.html)"],"metadata":{}},{"cell_type":"markdown","source":["##Project Formulation and Hypotheses <a name=\"introduction\"></a>"],"metadata":{}},{"cell_type":"markdown","source":["The dataset chosen was the AllState Insurance claims dataset. \n\nThe **goal of the analysis is to correctly train an algorithm to accurately predict the severity of an insurance claim** based on certain variable inputs. From insurance company's perspective, being able to accurately anticipate future claims and their severity can help financial planning and set up sufficient reserve amount to back it up. It also helps foresee any catastrophic events that may impede their financial well-being. \n\nAs far as the data provided, there were three separate datasets, which included both categorical and continuous data fields. \nThe datasets given were a training set, a test set, and a sample output. \n\nBecause the target variable is the severity of claims, we are labeling this as a regression problem, and as such, the data given will be fed through various algorithms and evaluated based on the MAE (mean absolute error), which is a linear score using equal weighting for all individual differences. In order to justify an accurately performing model, we will be searching for the lowest MAE score associated. The most optimal algorithm will also have the lowest bias and variance, which relate to the model’s ability to fit the training and test set, respectively. Finding the best trade-off between these two terms is important because the lower the bias, the smaller the error, but the higher chance for model complexity, whereas only focusing on a lower variance could result in underfitting if the complexity is too simple; important features may be missed.\n\nWe will implement two pipelines: sklearn pipeline and spark pipeline."],"metadata":{}},{"cell_type":"markdown","source":["#### Setup Code"],"metadata":{}},{"cell_type":"code","source":["# imports\nimport re\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ast\nimport os\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error \nfrom numpy import mean\nfrom numpy import absolute\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\nuserhome = 'dbfs:/user/' + username\nprint(userhome)\nfinalproject_path = userhome + \"/finalproject/\" \nfinalproject_path_open = '/dbfs' + finalproject_path.split(':')[-1] # for use with python open()\ndbutils.fs.mkdirs(finalproject_path)\n\nsum = 0\nDATA_PATH = 'dbfs:/mnt/mids-w261/data/datasets_final_project/'\nfor item in dbutils.fs.ls(DATA_PATH):\n  sum = sum+item.size\nsum"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dbfs:/user/sophia@ischool.berkeley.edu\nOut[2]: 5690917077</div>"]}}],"execution_count":6},{"cell_type":"code","source":["sc = spark.sparkContext\nspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.74.237.4:40282\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.74.237.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":7},{"cell_type":"markdown","source":["## Data Storage and Scalability Exploration"],"metadata":{}},{"cell_type":"code","source":["#unzip files\nimport zipfile\nwith zipfile.ZipFile('/dbfs/mnt/mids-w261/data/datasets_final_project/allstate-claims-severity.zip') as zip_ref:\n    zip_ref.extractall(\"/dbfs/user/\"+username+\"/finalproject/\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["dbutils.fs.put(finalproject_path+'test.txt',\"hello world\",True)\ndisplay(dbutils.fs.ls(finalproject_path))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/sample_submission.csv</td><td>sample_submission.csv</td><td>1106039</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/sample_submission.csv.zip</td><td>sample_submission.csv.zip</td><td>296933</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/test.csv</td><td>test.csv</td><td>45715862</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/test.csv.zip</td><td>test.csv.zip</td><td>9873043</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/test.txt</td><td>test.txt</td><td>11</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/train.csv</td><td>train.csv</td><td>70025339</td></tr><tr><td>dbfs:/user/sophia@ischool.berkeley.edu/finalproject/train.csv.zip</td><td>train.csv.zip</td><td>15848282</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["**File descriptions:**\n\n- train.csv - the training set\n- test.csv - the test set. You must predict the loss value for the ids in this file.\n- sample_submission.csv - a sample submission file in the correct format\n\nUpdate:\n- Confirmed that test set does not have target variable 'loss' - the given test set does not have a target variable, so we will discard and split a holdout set from train set"],"metadata":{}},{"cell_type":"code","source":["trainheaders = dbutils.fs.head(finalproject_path + '/train.csv')\ntrainheaders = trainheaders.split('\\n')[0]\ntestheaders = dbutils.fs.head(finalproject_path + '/test.csv')\ntestheaders = testheaders.split('\\n')[0]\nprint(trainheaders)\nprint(testheaders)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 65536 bytes]\n[Truncated to first 65536 bytes]\nid,cat1,cat2,cat3,cat4,cat5,cat6,cat7,cat8,cat9,cat10,cat11,cat12,cat13,cat14,cat15,cat16,cat17,cat18,cat19,cat20,cat21,cat22,cat23,cat24,cat25,cat26,cat27,cat28,cat29,cat30,cat31,cat32,cat33,cat34,cat35,cat36,cat37,cat38,cat39,cat40,cat41,cat42,cat43,cat44,cat45,cat46,cat47,cat48,cat49,cat50,cat51,cat52,cat53,cat54,cat55,cat56,cat57,cat58,cat59,cat60,cat61,cat62,cat63,cat64,cat65,cat66,cat67,cat68,cat69,cat70,cat71,cat72,cat73,cat74,cat75,cat76,cat77,cat78,cat79,cat80,cat81,cat82,cat83,cat84,cat85,cat86,cat87,cat88,cat89,cat90,cat91,cat92,cat93,cat94,cat95,cat96,cat97,cat98,cat99,cat100,cat101,cat102,cat103,cat104,cat105,cat106,cat107,cat108,cat109,cat110,cat111,cat112,cat113,cat114,cat115,cat116,cont1,cont2,cont3,cont4,cont5,cont6,cont7,cont8,cont9,cont10,cont11,cont12,cont13,cont14,loss\nid,cat1,cat2,cat3,cat4,cat5,cat6,cat7,cat8,cat9,cat10,cat11,cat12,cat13,cat14,cat15,cat16,cat17,cat18,cat19,cat20,cat21,cat22,cat23,cat24,cat25,cat26,cat27,cat28,cat29,cat30,cat31,cat32,cat33,cat34,cat35,cat36,cat37,cat38,cat39,cat40,cat41,cat42,cat43,cat44,cat45,cat46,cat47,cat48,cat49,cat50,cat51,cat52,cat53,cat54,cat55,cat56,cat57,cat58,cat59,cat60,cat61,cat62,cat63,cat64,cat65,cat66,cat67,cat68,cat69,cat70,cat71,cat72,cat73,cat74,cat75,cat76,cat77,cat78,cat79,cat80,cat81,cat82,cat83,cat84,cat85,cat86,cat87,cat88,cat89,cat90,cat91,cat92,cat93,cat94,cat95,cat96,cat97,cat98,cat99,cat100,cat101,cat102,cat103,cat104,cat105,cat106,cat107,cat108,cat109,cat110,cat111,cat112,cat113,cat114,cat115,cat116,cont1,cont2,cont3,cont4,cont5,cont6,cont7,cont8,cont9,cont10,cont11,cont12,cont13,cont14\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# load the raw data into an RDD\ntrainRDD = sc.textFile(finalproject_path + '/train.csv')\\\n            .filter(lambda x: x != trainheaders)\n             "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["training_data = sc.textFile(finalproject_path + \"/train.csv\")\nprint(training_data.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">188319\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["###Data Storage and Scalability\n\n####Summary Statistics\n\n####Data: \n- 188319 in train set and 125546 in test set. \n- There are no missing values. \n- 116 categorical data fields and 14 continuous data fields. \n- Continuous data fields are already normalized between 0 and 1. \n- Target variable values vary a lot with min of 0.67 and max of 121012. \n\n####Size\nThe dataset is not very large, at only 70MB or so for the training set, and 50MB or so for the test set, uncompressed. The counts for the dataset is 188k for the training set and 126k for the test set, small enough to run locally and large enough to load into memory. \n\nAssuming the dataset can't grow or be expanded upon to gain magnitudes in size, which for claims data, is unlikely, we can consider a few storage options:\n\n- CSV - the current storage format, easy to serialize/compress and cross compatible as input for many tools of analysis. reasonable file type for moving around a network, but prone to consistency and performance issues if used as a definitive source for query / update.\n- SQL - datastore that enables quick and consistent updates, queries, uptime, etc. good for pulling up rows of data and quick analysis and transforms.\n- RDD - distributed storage if we expect large volumes of claims in the long term future.\n- DataFrame/DataTable - a secondary storage format, great for quick analysis of historical numbers, easy to run locally for simple analysis\n\nAlso noted is that the data is very structured, which suits well for above data formats. If data was unstructured or semi-structured, we can consider more document based storage solutions. We have 116 categorical values and 14 continuous values.\n\n####Future Extensions of Data Consideration\nHowever, the size of this dataset can increase if:\n\n- the problem itself introduces more types and numbers of claims over time\n- the nature of the problem we consider includes more factors, such as IoT or smart device data linked to person(s) related to a claim\n\nFor those factors, we may consider a more scalable distributed storage solution with:\n\n- distributed SQL for well structured data columns to enable quick queries\n- distributed JSON (Document oriented DB like NoSQL, Mongo) for semi-structured device data\n\nBecause of the structured, small dataset, SQL could be a great storage solution and a serialized DataFrame or DataTable could be great for quick analysis. If the data grows in size, or is expanded upon, we can consider a more distributed solution.\n\nIn the interest of this class, we will stick to RDD operations and use Dataframes as a fallback/sanity check."],"metadata":{}},{"cell_type":"markdown","source":["##Exploratory Data Analysis"],"metadata":{}},{"cell_type":"markdown","source":["#### Data Skew\nThe EDA showed that the target variable is highly skewed, so a log transformation is needed. Transformation will mute the skewness. The other continuous values all had similar mean and variance. One issue that was noticed was that the test dataset provided was missing the target variable column. We will disgard test set and split out holdout set from train set.\n\n\nWe used histograms and a covariance matrix to review variables distribution and relation."],"metadata":{}},{"cell_type":"code","source":["#cache train set continuous values\ntrainRDDCached_cont=trainRDD.map(lambda x: x.split(','))\\\n        .map(lambda x: (x[117:131],x[-1])).cache()\ntotalRDDCached_cont = trainRDDCached_cont\none_cont = trainRDDCached_cont.take(1)[0]\none_cont"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: ([&#39;0.7263&#39;,\n  &#39;0.245921&#39;,\n  &#39;0.187583&#39;,\n  &#39;0.789639&#39;,\n  &#39;0.310061&#39;,\n  &#39;0.718367&#39;,\n  &#39;0.33506&#39;,\n  &#39;0.3026&#39;,\n  &#39;0.67135&#39;,\n  &#39;0.8351&#39;,\n  &#39;0.569745&#39;,\n  &#39;0.594646&#39;,\n  &#39;0.822493&#39;,\n  &#39;0.714843&#39;],\n &#39;2213.18&#39;)</div>"]}}],"execution_count":18},{"cell_type":"code","source":["#cache train set categorical values\ntrainRDDCached_cat=trainRDD.map(lambda x: x.split(','))\\\n        .map(lambda x: (x[1:116],x[-1])).cache()\ntotalRDDCached_cat = trainRDDCached_cat\none_cat = trainRDDCached_cat.take(1)[0]\none_cat"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: ([&#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;D&#39;,\n  &#39;B&#39;,\n  &#39;B&#39;,\n  &#39;D&#39;,\n  &#39;D&#39;,\n  &#39;B&#39;,\n  &#39;D&#39;,\n  &#39;C&#39;,\n  &#39;B&#39;,\n  &#39;D&#39;,\n  &#39;B&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;D&#39;,\n  &#39;B&#39;,\n  &#39;C&#39;,\n  &#39;E&#39;,\n  &#39;A&#39;,\n  &#39;C&#39;,\n  &#39;T&#39;,\n  &#39;B&#39;,\n  &#39;G&#39;,\n  &#39;A&#39;,\n  &#39;A&#39;,\n  &#39;I&#39;,\n  &#39;E&#39;,\n  &#39;G&#39;,\n  &#39;J&#39;,\n  &#39;G&#39;,\n  &#39;BU&#39;,\n  &#39;BC&#39;,\n  &#39;C&#39;,\n  &#39;AS&#39;,\n  &#39;S&#39;,\n  &#39;A&#39;,\n  &#39;O&#39;],\n &#39;2213.18&#39;)</div>"]}}],"execution_count":19},{"cell_type":"code","source":["#convert RDD to dataframe\ndataset_cont = np.array(totalRDDCached_cont.map(lambda x: np.append(x[0], [x[1]])).take(188318))\ndataset_cat = np.array(totalRDDCached_cat.map(lambda x: np.append(x[0], [x[1]])).take(188318))\n                                \nFIELDS_continuous = trainheaders.split(',')[117:132]\nFIELDS_cat = trainheaders.split(',')[1:116]\nFIELDS_cat.append('loss')\ndataset_cont_df = pd.DataFrame(np.array(dataset_cont),columns=FIELDS_continuous)\ndataset_cat_df = pd.DataFrame(np.array(dataset_cat),columns=FIELDS_cat)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["dataset_cont_df = dataset_cont_df.convert_objects(convert_numeric=True)\ndataset_cont_df.info()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1586861669172-0/PythonShell.py:1: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\nFor all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n  from __future__ import absolute_import\n&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 188318 entries, 0 to 188317\nData columns (total 15 columns):\ncont1     188318 non-null float64\ncont2     188318 non-null float64\ncont3     188318 non-null float64\ncont4     188318 non-null float64\ncont5     188318 non-null float64\ncont6     188318 non-null float64\ncont7     188318 non-null float64\ncont8     188318 non-null float64\ncont9     188318 non-null float64\ncont10    188318 non-null float64\ncont11    188318 non-null float64\ncont12    188318 non-null float64\ncont13    188318 non-null float64\ncont14    188318 non-null float64\nloss      188318 non-null float64\ndtypes: float64(15)\nmemory usage: 21.6 MB\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["The continuous variables are all converted to numeric data type. All of them have filled values between 0 and 1 except target variable 'loss'.\nThe continuous independent variables have similar mean and standard deviation. There are no extreme values or missing values."],"metadata":{}},{"cell_type":"code","source":["dataset_cont_df.describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cont1</th>\n      <th>cont2</th>\n      <th>cont3</th>\n      <th>cont4</th>\n      <th>cont5</th>\n      <th>cont6</th>\n      <th>cont7</th>\n      <th>cont8</th>\n      <th>cont9</th>\n      <th>cont10</th>\n      <th>cont11</th>\n      <th>cont12</th>\n      <th>cont13</th>\n      <th>cont14</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.493861</td>\n      <td>0.507188</td>\n      <td>0.498918</td>\n      <td>0.491812</td>\n      <td>0.487428</td>\n      <td>0.490945</td>\n      <td>0.484970</td>\n      <td>0.486437</td>\n      <td>0.485506</td>\n      <td>0.498066</td>\n      <td>0.493511</td>\n      <td>0.493150</td>\n      <td>0.493138</td>\n      <td>0.495717</td>\n      <td>3037.337686</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.187640</td>\n      <td>0.207202</td>\n      <td>0.202105</td>\n      <td>0.211292</td>\n      <td>0.209027</td>\n      <td>0.205273</td>\n      <td>0.178450</td>\n      <td>0.199370</td>\n      <td>0.181660</td>\n      <td>0.185877</td>\n      <td>0.209737</td>\n      <td>0.209427</td>\n      <td>0.212777</td>\n      <td>0.222488</td>\n      <td>2904.086186</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000016</td>\n      <td>0.001149</td>\n      <td>0.002634</td>\n      <td>0.176921</td>\n      <td>0.281143</td>\n      <td>0.012683</td>\n      <td>0.069503</td>\n      <td>0.236880</td>\n      <td>0.000080</td>\n      <td>0.000000</td>\n      <td>0.035321</td>\n      <td>0.036232</td>\n      <td>0.000228</td>\n      <td>0.179722</td>\n      <td>0.670000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.346090</td>\n      <td>0.358319</td>\n      <td>0.336963</td>\n      <td>0.327354</td>\n      <td>0.281143</td>\n      <td>0.336105</td>\n      <td>0.350175</td>\n      <td>0.312800</td>\n      <td>0.358970</td>\n      <td>0.364580</td>\n      <td>0.310961</td>\n      <td>0.311661</td>\n      <td>0.315758</td>\n      <td>0.294610</td>\n      <td>1204.460000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.475784</td>\n      <td>0.555782</td>\n      <td>0.527991</td>\n      <td>0.452887</td>\n      <td>0.422268</td>\n      <td>0.440945</td>\n      <td>0.438285</td>\n      <td>0.441060</td>\n      <td>0.441450</td>\n      <td>0.461190</td>\n      <td>0.457203</td>\n      <td>0.462286</td>\n      <td>0.363547</td>\n      <td>0.407403</td>\n      <td>2115.570000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.623912</td>\n      <td>0.681761</td>\n      <td>0.634224</td>\n      <td>0.652072</td>\n      <td>0.643315</td>\n      <td>0.655021</td>\n      <td>0.591045</td>\n      <td>0.623580</td>\n      <td>0.566820</td>\n      <td>0.614590</td>\n      <td>0.678924</td>\n      <td>0.675759</td>\n      <td>0.689974</td>\n      <td>0.724623</td>\n      <td>3864.045000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.984975</td>\n      <td>0.862654</td>\n      <td>0.944251</td>\n      <td>0.954297</td>\n      <td>0.983674</td>\n      <td>0.997162</td>\n      <td>1.000000</td>\n      <td>0.980200</td>\n      <td>0.995400</td>\n      <td>0.994980</td>\n      <td>0.998742</td>\n      <td>0.998484</td>\n      <td>0.988494</td>\n      <td>0.844848</td>\n      <td>121012.250000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["dataset_cat[0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: array([&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;D&#39;, &#39;B&#39;,\n       &#39;B&#39;, &#39;D&#39;, &#39;D&#39;, &#39;B&#39;, &#39;D&#39;, &#39;C&#39;, &#39;B&#39;, &#39;D&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;,\n       &#39;A&#39;, &#39;D&#39;, &#39;B&#39;, &#39;C&#39;, &#39;E&#39;, &#39;A&#39;, &#39;C&#39;, &#39;T&#39;, &#39;B&#39;, &#39;G&#39;, &#39;A&#39;, &#39;A&#39;, &#39;I&#39;,\n       &#39;E&#39;, &#39;G&#39;, &#39;J&#39;, &#39;G&#39;, &#39;BU&#39;, &#39;BC&#39;, &#39;C&#39;, &#39;AS&#39;, &#39;S&#39;, &#39;A&#39;, &#39;O&#39;,\n       &#39;2213.18&#39;], dtype=&#39;&lt;U9&#39;)</div>"]}}],"execution_count":24},{"cell_type":"code","source":["dataset_cat_df.info()\nfor col in dataset_cat_df.columns.values:\n  if (col != 'loss'):\n    dataset_cat_df[col] = dataset_cat_df[col].astype('category')\n  else:\n    dataset_cat_df[col] = dataset_cat_df[col].astype('float')\n\ndataset_cat_df.info()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 188318 entries, 0 to 188317\nColumns: 116 entries, cat1 to loss\ndtypes: object(116)\nmemory usage: 166.7+ MB\n&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 188318 entries, 0 to 188317\nColumns: 116 entries, cat1 to loss\ndtypes: category(115), float64(1)\nmemory usage: 22.3 MB\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["cat_columns = {}\ncat_columns_total = 0\nfor col in dataset_cat_df.columns.values:\n  if (col != 'loss'):\n    val_count = dataset_cat_df[col].value_counts()\n    cat_columns_total += len(val_count)\n    for key, count_ in enumerate(val_count):\n      if (col not in cat_columns):\n        cat_columns[col] = []\n      cat_columns[col].append(count_)\n\nprint(cat_columns)\nprint(cat_columns_total)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;cat1&#39;: [141550, 46768], &#39;cat2&#39;: [106721, 81597], &#39;cat3&#39;: [177993, 10325], &#39;cat4&#39;: [128395, 59923], &#39;cat5&#39;: [123737, 64581], &#39;cat6&#39;: [131693, 56625], &#39;cat7&#39;: [183744, 4574], &#39;cat8&#39;: [177274, 11044], &#39;cat9&#39;: [113122, 75196], &#39;cat10&#39;: [160213, 28105], &#39;cat11&#39;: [168186, 20132], &#39;cat12&#39;: [159825, 28493], &#39;cat13&#39;: [168851, 19467], &#39;cat14&#39;: [186041, 2277], &#39;cat15&#39;: [188284, 34], &#39;cat16&#39;: [181843, 6475], &#39;cat17&#39;: [187009, 1309], &#39;cat18&#39;: [187331, 987], &#39;cat19&#39;: [186510, 1808], &#39;cat20&#39;: [188114, 204], &#39;cat21&#39;: [187905, 413], &#39;cat22&#39;: [188275, 43], &#39;cat23&#39;: [157445, 30873], &#39;cat24&#39;: [181977, 6341], &#39;cat25&#39;: [169969, 18349], &#39;cat26&#39;: [177119, 11199], &#39;cat27&#39;: [168250, 20068], &#39;cat28&#39;: [180938, 7380], &#39;cat29&#39;: [184593, 3725], &#39;cat30&#39;: [184760, 3558], &#39;cat31&#39;: [182980, 5338], &#39;cat32&#39;: [187107, 1211], &#39;cat33&#39;: [187361, 957], &#39;cat34&#39;: [187734, 584], &#39;cat35&#39;: [188105, 213], &#39;cat36&#39;: [156313, 32005], &#39;cat37&#39;: [165729, 22589], &#39;cat38&#39;: [169323, 18995], &#39;cat39&#39;: [183393, 4925], &#39;cat40&#39;: [180119, 8199], &#39;cat41&#39;: [181177, 7141], &#39;cat42&#39;: [186623, 1695], &#39;cat43&#39;: [184110, 4208], &#39;cat44&#39;: [172716, 15602], &#39;cat45&#39;: [183991, 4327], &#39;cat46&#39;: [187436, 882], &#39;cat47&#39;: [187617, 701], &#39;cat48&#39;: [188049, 269], &#39;cat49&#39;: [179127, 9191], &#39;cat50&#39;: [137611, 50707], &#39;cat51&#39;: [187071, 1247], &#39;cat52&#39;: [179505, 8813], &#39;cat53&#39;: [172949, 15369], &#39;cat54&#39;: [183762, 4556], &#39;cat55&#39;: [188173, 145], &#39;cat56&#39;: [188136, 182], &#39;cat57&#39;: [185296, 3022], &#39;cat58&#39;: [188079, 239], &#39;cat59&#39;: [188018, 300], &#39;cat60&#39;: [187872, 446], &#39;cat61&#39;: [187596, 722], &#39;cat62&#39;: [188273, 45], &#39;cat63&#39;: [188239, 79], &#39;cat64&#39;: [188271, 47], &#39;cat65&#39;: [186056, 2262], &#39;cat66&#39;: [179982, 8336], &#39;cat67&#39;: [187626, 692], &#39;cat68&#39;: [188176, 142], &#39;cat69&#39;: [188011, 307], &#39;cat70&#39;: [188295, 23], &#39;cat71&#39;: [178646, 9672], &#39;cat72&#39;: [118322, 69996], &#39;cat73&#39;: [154275, 34017, 26], &#39;cat74&#39;: [184731, 3561, 26], &#39;cat75&#39;: [154307, 34010, 1], &#39;cat76&#39;: [181347, 6183, 788], &#39;cat77&#39;: [187503, 408, 358, 49], &#39;cat78&#39;: [186526, 788, 645, 359], &#39;cat79&#39;: [152929, 26657, 7064, 1668], &#39;cat80&#39;: [137505, 46538, 3492, 783], &#39;cat81&#39;: [154385, 24132, 9013, 788], &#39;cat82&#39;: [147536, 19322, 18805, 2655], &#39;cat83&#39;: [141534, 26038, 15788, 4958], &#39;cat84&#39;: [154939, 29450, 3498, 431], &#39;cat85&#39;: [186005, 1011, 788, 514], &#39;cat86&#39;: [103852, 72587, 10290, 1589], &#39;cat87&#39;: [166992, 11719, 8819, 788], &#39;cat88&#39;: [168926, 19302, 83, 7], &#39;cat89&#39;: [183744, 4312, 220, 33, 5, 2, 1, 1], &#39;cat90&#39;: [177993, 9515, 728, 70, 6, 4, 2], &#39;cat91&#39;: [111028, 42630, 26734, 6400, 1149, 254, 97, 26], &#39;cat92&#39;: [124689, 62901, 628, 62, 26, 11, 1], &#39;cat93&#39;: [150237, 35788, 1133, 728, 432], &#39;cat94&#39;: [121642, 51710, 13623, 738, 494, 91, 20], &#39;cat95&#39;: [87531, 79525, 17417, 3736, 109], &#39;cat96&#39;: [174360, 7922, 2957, 2665, 343, 35, 24, 12], &#39;cat97&#39;: [78127, 47450, 41970, 16745, 3779, 213, 34], &#39;cat98&#39;: [105492, 50557, 21485, 10242, 542], &#39;cat99&#39;: [79455, 72591, 10290, 8844, 7045, 2894, 2703, 2702, 1034, 310, 245, 93, 52, 38, 19, 3], &#39;cat100&#39;: [42970, 39933, 19961, 13817, 12935, 12027, 10776, 9402, 7592, 6608, 5697, 5185, 836, 553, 26], &#39;cat101&#39;: [106721, 17171, 16971, 10944, 10139, 7259, 6690, 3669, 3173, 2762, 2493, 173, 138, 7, 3, 2, 1, 1, 1], &#39;cat102&#39;: [177274, 5155, 4929, 482, 449, 15, 12, 1, 1], &#39;cat103&#39;: [123737, 33342, 16508, 7806, 4473, 1528, 550, 190, 109, 45, 17, 11, 2], &#39;cat104&#39;: [42925, 40660, 27611, 19228, 17187, 14297, 10919, 6949, 3486, 3138, 1156, 409, 153, 86, 68, 43, 3], &#39;cat105&#39;: [76493, 62892, 20613, 12172, 11258, 2941, 694, 486, 275, 171, 145, 77, 64, 15, 9, 5, 4, 2, 1, 1], &#39;cat106&#39;: [47165, 37713, 36143, 21433, 18281, 13000, 7958, 2971, 1934, 1176, 231, 129, 75, 66, 32, 9, 2], &#39;cat107&#39;: [47310, 28560, 23461, 22405, 20236, 20066, 12521, 6976, 3225, 2067, 797, 213, 140, 125, 100, 75, 32, 5, 2, 2], &#39;cat108&#39;: [65512, 42435, 21421, 19160, 10242, 9299, 7968, 7243, 4305, 520, 213], &#39;cat109&#39;: [152918, 21933, 3142, 2999, 1353, 1067, 461, 446, 329, 257, 214, 208, 200, 178, 159, 151, 149, 143, 124, 122, 109, 94, 89, 78, 74, 73, 66, 66, 61, 59, 58, 57, 55, 54, 52, 52, 52, 50, 49, 43, 43, 39, 33, 25, 23, 22, 22, 18, 17, 17, 16, 15, 14, 14, 14, 13, 12, 11, 9, 9, 8, 8, 8, 8, 8, 7, 6, 6, 5, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1], &#39;cat110&#39;: [25305, 24654, 24592, 21396, 17495, 16365, 9236, 4131, 3640, 3271, 3096, 2965, 2867, 2703, 2681, 2611, 1925, 1920, 1745, 1692, 1296, 1185, 1159, 1066, 796, 767, 643, 626, 576, 529, 395, 343, 336, 296, 236, 233, 222, 206, 202, 181, 176, 175, 153, 146, 138, 121, 120, 104, 90, 72, 70, 66, 62, 61, 56, 52, 52, 48, 39, 39, 38, 38, 35, 34, 31, 29, 27, 27, 26, 26, 26, 22, 22, 21, 21, 20, 20, 20, 20, 19, 19, 18, 17, 17, 16, 15, 15, 15, 14, 14, 14, 13, 12, 12, 11, 11, 10, 10, 9, 8, 8, 8, 8, 8, 7, 6, 6, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], &#39;cat111&#39;: [128395, 32401, 14682, 7039, 3578, 1353, 473, 221, 91, 38, 16, 16, 7, 3, 3, 2], &#39;cat112&#39;: [25148, 18639, 17669, 16222, 9368, 9138, 8453, 8356, 7122, 6726, 6059, 4749, 4201, 4000, 3168, 3149, 3145, 2411, 2365, 2257, 2183, 1645, 1531, 1414, 1351, 1331, 1272, 1241, 1170, 1130, 1123, 1074, 940, 925, 834, 793, 693, 548, 534, 521, 493, 461, 454, 439, 434, 423, 406, 246, 190, 144, 30], &#39;cat113&#39;: [26191, 22030, 13058, 12661, 11374, 7738, 7033, 7016, 6079, 5214, 5094, 4664, 4425, 4419, 3901, 3520, 3486, 3279, 3269, 3105, 2839, 2793, 2784, 2605, 2485, 2415, 1825, 1804, 1630, 1628, 1463, 1413, 1221, 693, 528, 466, 450, 440, 347, 337, 209, 143, 53, 42, 32, 18, 18, 17, 15, 11, 10, 7, 6, 3, 3, 3, 2, 1, 1, 1, 1], &#39;cat114&#39;: [131693, 16793, 16475, 8199, 7905, 2455, 2432, 912, 870, 250, 240, 43, 33, 6, 5, 4, 1, 1, 1], &#39;cat115&#39;: [43866, 26813, 23895, 22438, 21538, 16125, 12444, 8258, 7090, 2793, 2038, 315, 269, 250, 75, 56, 26, 11, 6, 5, 4, 2, 1]}\n813\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["for the continuous values, some are left skewed and some are right skewed."],"metadata":{}},{"cell_type":"code","source":["dataset_cont_df.astype('float').hist(figsize=(15,15), bins=15)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Target variable 'loss' is highly skewed based on its histogram. We will transform it using np.log1p to smooth it out. It also has extreme values, extremely small (<1) or extremely large (>100,000). We will exclude them from regression."],"metadata":{}},{"cell_type":"code","source":["print(dataset_cont_df.astype('float').skew())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">cont1     0.516424\ncont2    -0.310941\ncont3    -0.010002\ncont4     0.416096\ncont5     0.681622\ncont6     0.461214\ncont7     0.826053\ncont8     0.676634\ncont9     1.072429\ncont10    0.355001\ncont11    0.280821\ncont12    0.291992\ncont13    0.380742\ncont14    0.248674\nloss      3.794958\ndtype: float64\n</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["target variable follows more like normal distribution after log transformation"],"metadata":{}},{"cell_type":"code","source":["plt.figure(figsize=(7,5))\nsns.distplot(np.log1p(dataset_cont_df[FIELDS_continuous[-1]].astype('float')))\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Amongst the continuous variables, some are very correlated (darker red). Highly correlated independent variables would influence regression results as the assumption is that all independent variables are not correlated. We will use PCA to make all variables othogonal."],"metadata":{}},{"cell_type":"code","source":["corr = dataset_cont_df[FIELDS_continuous[:-1]].astype('float').corr()\nfig, ax = plt.subplots(figsize=(9, 7))\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\nplt.title(\"Correlations between features.\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["##Proposed Features\n\n####Variable Dimensions\nWe have 116 categorical values and 14 continuous values in addition to our loss outcome variable. Exploring the categorical variables, they can be expanded into 813 dummy variables or dimensions. Some of the categorical variables are binary (only two values) and others go up to dozens of values with a skewed distribution, e.g. cat113, cat115. Given that there are 180k data rows, 813 variables is significant, and likely needs to be pruned. However, its not such a large number of dimensions that a kitchen sink model is out of the question.\n\nSome of the dummy variables are positively correlated or negatively correlated with loss, as well as the continuous variables. The dummy and continuous variables are inter-correlated as well.\n\nWe are not given hints about any of the variables underlying meaning or context, although some of them are likely gender, age, etc. In this sense, we will focus on model creation that minimizes error (type I and type II) as well as minimizing the number of inputs. We will cross validate with test sets.\n\n####Potential Approaches\n\n- Brute force use all variables (all dummies + continuous), and refine model by gradient descent with Lasso/Ridge to minimize model complexity\n- PCA / Dimensionality reduction prior to model construction\n- Transformation on target variable\n\nA baseline linear regression model (kitchen sink) is not a bad idea to draw a baseline."],"metadata":{}},{"cell_type":"markdown","source":["#### Feature Engineering\n\n##### One-hot Encoding\nCategorical variables are likely related to an individual’s characteristics, for example, sex, ethnicity, age, etc., whereas continuous variables may relate to height, weight, or the amount of time since the individual’s last traffic incident.\n\n##### Log-Transform Output Variable\nWe logged transformed the output variable since it showed a very high skew, and the normalization by log1 tranformed the output variable into a normal curve.\n\n##### Principal Component Analysis\nFor the task of feature engineering, PCA (Principal Component Analysis), which is an “unsupervised, non-parametric statistical technique primarily used for dimensionality reduction,” (source: https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db) was used as an attempt to whittle down the amount of input variables. Often times, when there are too many features, sometimes referred to as high-dimensionality, we run into the issue of model overfitting. If we overfit a model to the training set, then the model is limited to fitting only those scenarios found in the training set and will likely have a much higher error rate on a test set with even a few new scenarios."],"metadata":{}},{"cell_type":"markdown","source":["Step 1: encoding all categorical variables using get_dummies"],"metadata":{}},{"cell_type":"code","source":["# encoding for all cat variables\ndataset_cat_df_dummies = pd.get_dummies(dataset_cat_df, columns=['cat1', 'cat2','cat3', 'cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18','cat19','cat20','cat21','cat22','cat23','cat24','cat25','cat26','cat27','cat28','cat29','cat30','cat31','cat32','cat33','cat34','cat35','cat36','cat37','cat38','cat39','cat40','cat41','cat42','cat43','cat44','cat45', 'cat46','cat47','cat48','cat49','cat50','cat51','cat52','cat53','cat54','cat55','cat56','cat57','cat58','cat59','cat60','cat61','cat62','cat63','cat64','cat65', 'cat66','cat67','cat68','cat69','cat70','cat71','cat72','cat73','cat74','cat75','cat76','cat77','cat78','cat79','cat80','cat81','cat82','cat83','cat84','cat85', 'cat86','cat87','cat88','cat89','cat90','cat91','cat92','cat93','cat94','cat95','cat96','cat97','cat98','cat99','cat100','cat101','cat102','cat103','cat104','cat105', 'cat106','cat107','cat108','cat109','cat110','cat111','cat112','cat113','cat114','cat115'])\ndataset_cat_df_dummies.describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>cat1_A</th>\n      <th>cat1_B</th>\n      <th>cat2_A</th>\n      <th>cat2_B</th>\n      <th>cat3_A</th>\n      <th>cat3_B</th>\n      <th>cat4_A</th>\n      <th>cat4_B</th>\n      <th>cat5_A</th>\n      <th>cat5_B</th>\n      <th>cat6_A</th>\n      <th>cat6_B</th>\n      <th>cat7_A</th>\n      <th>cat7_B</th>\n      <th>cat8_A</th>\n      <th>cat8_B</th>\n      <th>cat9_A</th>\n      <th>cat9_B</th>\n      <th>cat10_A</th>\n      <th>cat10_B</th>\n      <th>cat11_A</th>\n      <th>cat11_B</th>\n      <th>cat12_A</th>\n      <th>cat12_B</th>\n      <th>cat13_A</th>\n      <th>cat13_B</th>\n      <th>cat14_A</th>\n      <th>cat14_B</th>\n      <th>cat15_A</th>\n      <th>cat15_B</th>\n      <th>cat16_A</th>\n      <th>cat16_B</th>\n      <th>cat17_A</th>\n      <th>cat17_B</th>\n      <th>cat18_A</th>\n      <th>cat18_B</th>\n      <th>cat19_A</th>\n      <th>cat19_B</th>\n      <th>cat20_A</th>\n      <th>...</th>\n      <th>cat114_C</th>\n      <th>cat114_D</th>\n      <th>cat114_E</th>\n      <th>cat114_F</th>\n      <th>cat114_G</th>\n      <th>cat114_I</th>\n      <th>cat114_J</th>\n      <th>cat114_L</th>\n      <th>cat114_N</th>\n      <th>cat114_O</th>\n      <th>cat114_Q</th>\n      <th>cat114_R</th>\n      <th>cat114_S</th>\n      <th>cat114_U</th>\n      <th>cat114_V</th>\n      <th>cat114_W</th>\n      <th>cat114_X</th>\n      <th>cat115_A</th>\n      <th>cat115_B</th>\n      <th>cat115_C</th>\n      <th>cat115_D</th>\n      <th>cat115_E</th>\n      <th>cat115_F</th>\n      <th>cat115_G</th>\n      <th>cat115_H</th>\n      <th>cat115_I</th>\n      <th>cat115_J</th>\n      <th>cat115_K</th>\n      <th>cat115_L</th>\n      <th>cat115_M</th>\n      <th>cat115_N</th>\n      <th>cat115_O</th>\n      <th>cat115_P</th>\n      <th>cat115_Q</th>\n      <th>cat115_R</th>\n      <th>cat115_S</th>\n      <th>cat115_T</th>\n      <th>cat115_U</th>\n      <th>cat115_W</th>\n      <th>cat115_X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>...</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n      <td>188318.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3037.337686</td>\n      <td>0.751654</td>\n      <td>0.248346</td>\n      <td>0.566706</td>\n      <td>0.433294</td>\n      <td>0.945173</td>\n      <td>0.054827</td>\n      <td>0.681799</td>\n      <td>0.318201</td>\n      <td>0.657064</td>\n      <td>0.342936</td>\n      <td>0.699312</td>\n      <td>0.300688</td>\n      <td>0.975711</td>\n      <td>0.024289</td>\n      <td>0.941355</td>\n      <td>0.058645</td>\n      <td>0.600697</td>\n      <td>0.399303</td>\n      <td>0.850758</td>\n      <td>0.149242</td>\n      <td>0.893096</td>\n      <td>0.106904</td>\n      <td>0.848697</td>\n      <td>0.151303</td>\n      <td>0.896627</td>\n      <td>0.103373</td>\n      <td>0.987909</td>\n      <td>0.012091</td>\n      <td>0.999819</td>\n      <td>0.000181</td>\n      <td>0.965617</td>\n      <td>0.034383</td>\n      <td>0.993049</td>\n      <td>0.006951</td>\n      <td>0.994759</td>\n      <td>0.005241</td>\n      <td>0.990399</td>\n      <td>0.009601</td>\n      <td>0.998917</td>\n      <td>...</td>\n      <td>0.089174</td>\n      <td>0.000027</td>\n      <td>0.087485</td>\n      <td>0.041977</td>\n      <td>0.000005</td>\n      <td>0.012914</td>\n      <td>0.043538</td>\n      <td>0.004620</td>\n      <td>0.013036</td>\n      <td>0.001274</td>\n      <td>0.000228</td>\n      <td>0.004843</td>\n      <td>0.000021</td>\n      <td>0.001328</td>\n      <td>0.000175</td>\n      <td>0.000005</td>\n      <td>0.000005</td>\n      <td>0.000398</td>\n      <td>0.000011</td>\n      <td>0.000005</td>\n      <td>0.000021</td>\n      <td>0.000058</td>\n      <td>0.001428</td>\n      <td>0.001673</td>\n      <td>0.014831</td>\n      <td>0.037649</td>\n      <td>0.126886</td>\n      <td>0.232936</td>\n      <td>0.085626</td>\n      <td>0.066080</td>\n      <td>0.119150</td>\n      <td>0.142382</td>\n      <td>0.114370</td>\n      <td>0.043851</td>\n      <td>0.010822</td>\n      <td>0.001328</td>\n      <td>0.000297</td>\n      <td>0.000138</td>\n      <td>0.000032</td>\n      <td>0.000027</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2904.086186</td>\n      <td>0.432055</td>\n      <td>0.432055</td>\n      <td>0.495532</td>\n      <td>0.495532</td>\n      <td>0.227644</td>\n      <td>0.227644</td>\n      <td>0.465779</td>\n      <td>0.465779</td>\n      <td>0.474692</td>\n      <td>0.474692</td>\n      <td>0.458559</td>\n      <td>0.458559</td>\n      <td>0.153944</td>\n      <td>0.153944</td>\n      <td>0.234961</td>\n      <td>0.234961</td>\n      <td>0.489757</td>\n      <td>0.489757</td>\n      <td>0.356328</td>\n      <td>0.356328</td>\n      <td>0.308992</td>\n      <td>0.308992</td>\n      <td>0.358345</td>\n      <td>0.358345</td>\n      <td>0.304446</td>\n      <td>0.304446</td>\n      <td>0.109294</td>\n      <td>0.109294</td>\n      <td>0.013436</td>\n      <td>0.013436</td>\n      <td>0.182212</td>\n      <td>0.182212</td>\n      <td>0.083083</td>\n      <td>0.083083</td>\n      <td>0.072206</td>\n      <td>0.072206</td>\n      <td>0.097512</td>\n      <td>0.097512</td>\n      <td>0.032895</td>\n      <td>...</td>\n      <td>0.284995</td>\n      <td>0.005153</td>\n      <td>0.282545</td>\n      <td>0.200537</td>\n      <td>0.002304</td>\n      <td>0.112905</td>\n      <td>0.204065</td>\n      <td>0.067812</td>\n      <td>0.113431</td>\n      <td>0.035677</td>\n      <td>0.015109</td>\n      <td>0.069422</td>\n      <td>0.004609</td>\n      <td>0.036411</td>\n      <td>0.013237</td>\n      <td>0.002304</td>\n      <td>0.002304</td>\n      <td>0.019953</td>\n      <td>0.003259</td>\n      <td>0.002304</td>\n      <td>0.004609</td>\n      <td>0.007643</td>\n      <td>0.037768</td>\n      <td>0.040865</td>\n      <td>0.120878</td>\n      <td>0.190347</td>\n      <td>0.332847</td>\n      <td>0.422703</td>\n      <td>0.279812</td>\n      <td>0.248422</td>\n      <td>0.323965</td>\n      <td>0.349442</td>\n      <td>0.318261</td>\n      <td>0.204765</td>\n      <td>0.103465</td>\n      <td>0.036411</td>\n      <td>0.017242</td>\n      <td>0.011749</td>\n      <td>0.005644</td>\n      <td>0.005153</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.670000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1204.460000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2115.570000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3864.045000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>121012.250000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 814 columns</p>\n</div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["Step 2: remove extreme values, merge continuous fields with categorical fields, and split train and test sets"],"metadata":{}},{"cell_type":"code","source":["dataset_cat_df_dummies = dataset_cat_df_dummies[dataset_cat_df_dummies['loss']<100000][dataset_cat_df_dummies['loss']>1]\ndataset_cont_df = dataset_cont_df[dataset_cont_df['loss']<100000][dataset_cont_df['loss']>1]\nX =  pd.concat([dataset_cat_df_dummies.loc[:,dataset_cat_df_dummies.columns != 'loss'], dataset_cont_df.loc[:,dataset_cont_df.columns != 'loss']], axis=1, sort=False)\ny = np.log1p(dataset_cont_df.loc[:,dataset_cont_df.columns == 'loss'].astype('float'))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1586861669172-0/PythonShell.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  from __future__ import absolute_import\n/local_disk0/tmp/1586861669172-0/PythonShell.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  from __future__ import print_function\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["y_train.describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>169483.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.684360</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.811080</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.832581</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.092981</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.655793</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.257392</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>11.361225</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["y_test.describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>18832.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.699792</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.810619</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.090588</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.109710</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.672507</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.278927</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10.635805</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["print(X_train.head())\nprint(X_train.shape)\nprint(X_test.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">        cat1_A  cat1_B  cat2_A  cat2_B  ...    cont11    cont12    cont13    cont14\n155565       0       1       0       1  ...  0.588753  0.630853  0.348267  0.377551\n20684        1       0       1       0  ...  0.757468  0.744640  0.804291  0.357100\n111660       1       0       0       1  ...  0.949450  0.944062  0.256038  0.413320\n26388        0       1       0       1  ...  0.640428  0.627291  0.342239  0.290703\n112695       0       1       1       0  ...  0.307628  0.301921  0.318646  0.561903\n\n[5 rows x 827 columns]\n(169483, 827)\n(18832, 827)\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["Run PCA to get an idea of how many dimension we need to get good explantory power. 200 dimension explains 96.6% variation."],"metadata":{}},{"cell_type":"code","source":["#Run PCA to identify important features\npca = PCA(n_components=200)\npca.fit(X_train)\nprint(pca.explained_variance_ratio_.cumsum())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[0.08403102 0.15587937 0.20347357 0.24180615 0.27542298 0.30517963\n 0.33320953 0.35896544 0.37916518 0.39783154 0.4157358  0.43265777\n 0.44828625 0.4631047  0.47689731 0.48865469 0.49959279 0.51031679\n 0.52046739 0.52997425 0.53932731 0.54824613 0.55711479 0.56569375\n 0.57415941 0.5821527  0.58981869 0.59727132 0.60456325 0.61167822\n 0.61869157 0.6255354  0.63216242 0.6383697  0.64446606 0.65037525\n 0.65625495 0.66206885 0.66726015 0.67237009 0.67740304 0.68236785\n 0.68723258 0.69201787 0.69671172 0.7011827  0.70552826 0.709821\n 0.71408023 0.71827899 0.7223249  0.7263441  0.73028748 0.73414612\n 0.73796379 0.74161015 0.74523392 0.7488252  0.75235059 0.7558491\n 0.75930669 0.7627125  0.76604902 0.76928536 0.77251105 0.7756526\n 0.77877778 0.78188432 0.7849158  0.78790096 0.79087902 0.79384072\n 0.79677724 0.79963219 0.80247453 0.80520138 0.80791269 0.81060891\n 0.81325919 0.81587126 0.81847326 0.82102156 0.82354957 0.82600573\n 0.82841767 0.83079803 0.83314217 0.83547412 0.83778378 0.84004994\n 0.842253   0.84443613 0.84659919 0.84874705 0.85087058 0.85295087\n 0.8550043  0.85703788 0.85903172 0.86101398 0.86298063 0.86490757\n 0.8668009  0.86867431 0.87051882 0.87231554 0.87407025 0.87580023\n 0.87752448 0.8792314  0.88089827 0.88255232 0.88419111 0.88579843\n 0.88739343 0.88896574 0.8905271  0.89207373 0.89359069 0.89509275\n 0.89658003 0.89805794 0.89952222 0.9009799  0.90240842 0.903793\n 0.90517469 0.9065396  0.90787586 0.90920274 0.91052247 0.91182862\n 0.91312282 0.91440258 0.91566048 0.91690287 0.91810003 0.91928057\n 0.92045259 0.92159896 0.92272383 0.92384115 0.92495582 0.92603045\n 0.92709325 0.92815385 0.92918567 0.93021677 0.93122728 0.93223055\n 0.93322459 0.9342115  0.93516545 0.93611045 0.93705169 0.93797785\n 0.93889775 0.93979359 0.9406618  0.94150986 0.94232707 0.94314272\n 0.94394904 0.94475042 0.94553623 0.94631255 0.94705056 0.94778095\n 0.94848589 0.94918485 0.94987791 0.95054115 0.95119518 0.951841\n 0.95248157 0.95311804 0.95374919 0.95437388 0.95499532 0.95559547\n 0.9561863  0.95677413 0.95735078 0.95792472 0.95848256 0.95903898\n 0.95958951 0.96013329 0.96066448 0.96118171 0.96169291 0.96219632\n 0.9626894  0.96317962 0.9636664  0.96414628 0.964618   0.96507562\n 0.96552839 0.96597803]\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["X_pca_train = pca.fit_transform(X_train)\nX_pca_test = pca.transform(X_test)\nprint(\"original shape:   \", X_train.shape)\nprint(\"transformed shape:\", X_pca_train.shape)\nprint(\"original shape:   \", X_test.shape)\nprint(\"transformed shape:\", X_pca_test.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">original shape:    (169483, 827)\ntransformed shape: (169483, 200)\noriginal shape:    (18832, 827)\ntransformed shape: (18832, 200)\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["Normalization: further transform target variable to make train/test on the same scale"],"metadata":{}},{"cell_type":"code","source":["scaler = MinMaxScaler()\ny_train_scaled = scaler.fit_transform(y_train)\ny_test_scaled = scaler.transform(y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":48},{"cell_type":"code","source":["pd.DataFrame(y_train_scaled).describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>169483.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.614125</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.085120</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.552062</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.611127</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.674263</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["pd.DataFrame(y_test_scaled).describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>18832.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.615745</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.085072</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.132024</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.553817</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.612881</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.676523</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.923870</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["Linear regression with PCA and scaled target variable"],"metadata":{}},{"cell_type":"code","source":["linear = LinearRegression()\nmodel = linear.fit(X_pca_train, y_train_scaled)\n\n#inverse scaling/transformation\ny_train_pred= np.expm1(scaler.inverse_transform(model.predict(X_pca_train)))\ny_test_pred= np.expm1(scaler.inverse_transform(model.predict(X_pca_test)))\nprint(mean_absolute_error(y_train_pred, np.expm1(y_train)))\nprint(mean_absolute_error(y_test_pred, np.expm1(y_test)))\n\nprint(model.coef_)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1252.1787004707478\n1271.9536992537749\n[[ 8.67694271e-03  2.16914708e-02  1.15647770e-04 -8.01385507e-05\n   8.21766795e-03  1.06207390e-02  3.77617419e-02  2.19307374e-02\n  -2.01410335e-03 -3.71711854e-03  7.18124590e-03 -1.37034457e-02\n   5.61059872e-03  2.85039736e-03 -2.75357039e-03 -7.11183955e-03\n   3.84892934e-03 -2.82096101e-03 -1.58224790e-02  7.00243621e-03\n   1.51274565e-02  6.04759640e-03  1.38659876e-02 -3.81442531e-04\n  -8.73160635e-03 -3.05114825e-04 -9.23007369e-03  3.90685766e-03\n  -7.37868341e-03  7.22047363e-03  3.54717193e-03  1.96123996e-02\n  -2.24065431e-04 -1.31393411e-03  7.65972554e-04 -4.45892080e-03\n   1.28591069e-02 -3.72391687e-03  2.85163867e-02  8.77512843e-03\n   1.29648905e-03 -7.96539165e-05 -3.49637603e-04  5.98912288e-03\n   4.36011841e-04  3.89771259e-03 -1.00111734e-02  4.45336451e-03\n  -8.10356761e-03 -7.38826985e-04 -2.73520830e-03  1.27213157e-02\n   1.01568472e-02 -2.62273186e-04 -2.73052887e-04 -4.69175947e-03\n  -6.58386709e-05 -8.03215401e-03 -1.49587152e-03 -4.13424079e-03\n  -7.18290204e-03 -3.80622066e-03  3.66286947e-03  9.29039209e-04\n  -1.53952773e-02 -8.04096686e-03  4.27926237e-03 -2.14488563e-03\n  -3.43408096e-03 -8.08162902e-03  2.34848509e-03 -6.57286697e-03\n   2.01770987e-03 -3.30827088e-03 -7.74887078e-03  2.10251727e-03\n  -6.52216864e-04  3.67141303e-04 -1.91813145e-03  1.58448160e-03\n  -5.40229630e-03  4.52729877e-03 -6.06454272e-03  2.54983504e-03\n   2.37816928e-03  1.49306785e-03  2.03618380e-03 -9.30315329e-05\n   8.03143925e-03 -4.37368480e-04 -6.20528631e-04  1.21067959e-02\n  -1.13157267e-03  1.08558164e-02  3.28272506e-03  1.10944787e-02\n  -2.04050618e-03 -7.83937231e-03  6.94114418e-03  1.26031109e-03\n   3.06625613e-03  2.50618662e-03 -3.38672973e-03  3.43744798e-03\n   1.11044417e-03  2.74603101e-03  4.85165551e-03 -3.93036981e-03\n  -8.02503188e-03 -1.60216504e-02 -6.90521643e-03  4.73982204e-03\n   8.38355754e-03 -8.18040972e-03 -2.19288172e-03  2.06880515e-03\n  -1.25746767e-02  1.14772358e-02 -4.82721202e-03 -1.27934414e-03\n  -3.79119274e-04 -2.81743227e-03 -1.02937991e-03 -3.42784169e-03\n  -9.70599405e-03 -5.39026175e-03 -8.39585209e-03 -1.13277641e-02\n  -8.76052708e-03 -6.02267713e-04  8.09823036e-04  9.69359616e-03\n   1.33235720e-02 -5.96865994e-03 -3.81093909e-03  5.95111212e-03\n  -1.76442376e-03  3.25126473e-03  1.52642201e-03 -2.66940852e-04\n   9.29875965e-04 -1.27409907e-03  1.19480393e-02  4.91114936e-04\n   2.26971787e-03 -2.74942427e-03 -1.89815008e-03  3.89573119e-04\n  -2.12310189e-03  3.49685919e-03  1.83595624e-02 -3.05836339e-04\n   1.24622100e-02  2.32830099e-03 -4.82269470e-03 -6.62801153e-03\n   2.35745194e-03 -1.39275787e-02  6.79544600e-03  2.57667417e-03\n   8.22349505e-03  5.07043034e-03 -5.68154123e-04 -1.01156748e-02\n  -6.92991613e-03  1.44001657e-03 -3.54706107e-03 -4.15335522e-03\n   6.44048500e-03 -4.20565711e-03  6.84923766e-03 -2.97040565e-03\n  -6.40090972e-03  9.48268160e-03  8.81952478e-03  2.47672995e-03\n  -6.23701096e-04 -6.95894568e-03  1.32378168e-02 -1.72646020e-02\n   5.74714813e-03  7.46699243e-04 -7.30920233e-03 -2.42903265e-04\n   6.01719455e-03 -7.58726023e-03 -1.70351507e-03  4.00237775e-03\n   4.30051562e-03 -6.02812453e-03 -3.18227661e-03 -4.46464376e-03\n  -1.48135290e-03  1.96834129e-03  1.58158186e-03  4.12514106e-03\n  -1.11497450e-02  1.60249114e-04 -3.12243986e-03 -1.42195774e-02]]\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["#########################################################\n# DF to RDD code \n#########################################################\n\ndef transformRDDtoDF(dataRDD):\n  \"\"\"helper function: transform dataRDD into dataframe with dummy variables.\"\"\"\n  dataRDDCached_cont=dataRDD.map(lambda x: x.split(','))\\\n    .map(lambda x: (x[117:131],x[-1])).cache()\n             \n  dataRDDCached_cat=dataRDD.map(lambda x: x.split(','))\\\n    .map(lambda x: (x[1:116],x[-1])).cache()\n\n  dataset_cont = np.array(dataRDDCached_cont.map(lambda x: np.append(x[0], [x[1]])).take(188318))\n  dataset_cat = np.array(dataRDDCached_cat.map(lambda x: np.append(x[0], [x[1]])).take(188318))\n\n  FIELDS_continuous = trainheaders.split(',')[117:132]\n  FIELDS_cat = trainheaders.split(',')[1:116]\n  FIELDS_cat.append('loss')\n  dataset_cont_df = pd.DataFrame(np.array(dataset_cont),columns=FIELDS_continuous)\n  dataset_cat_df = pd.DataFrame(np.array(dataset_cat),columns=FIELDS_cat)\n  dataset_cont_df = dataset_cont_df.convert_objects(convert_numeric=True)\n\n  for col in dataset_cat_df.columns.values:\n    if (col != 'loss'):\n      dataset_cat_df[col] = dataset_cat_df[col].astype('category')\n    else:\n      dataset_cat_df[col] = dataset_cat_df[col].astype('float')\n\n  \n  # encoding for all cat variables\n  dataset_cat_df_dummies = pd.get_dummies(dataset_cat_df, columns=['cat1', 'cat2','cat3', 'cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18','cat19','cat20','cat21','cat22','cat23','cat24','cat25','cat26','cat27','cat28','cat29','cat30','cat31','cat32','cat33','cat34','cat35','cat36','cat37','cat38','cat39','cat40','cat41','cat42','cat43','cat44','cat45', 'cat46','cat47','cat48','cat49','cat50','cat51','cat52','cat53','cat54','cat55','cat56','cat57','cat58','cat59','cat60','cat61','cat62','cat63','cat64','cat65', 'cat66','cat67','cat68','cat69','cat70','cat71','cat72','cat73','cat74','cat75','cat76','cat77','cat78','cat79','cat80','cat81','cat82','cat83','cat84','cat85', 'cat86','cat87','cat88','cat89','cat90','cat91','cat92','cat93','cat94','cat95','cat96','cat97','cat98','cat99','cat100','cat101','cat102','cat103','cat104','cat105', 'cat106','cat107','cat108','cat109','cat110','cat111','cat112','cat113','cat114','cat115'])\n\n  data_frame = pd.concat([dataset_cat_df_dummies.loc[:,dataset_cat_df_dummies.columns != 'loss'], dataset_cont_df.loc[:,dataset_cont_df.columns != 'loss']], axis=1, sort=False)\n  data_outcome = np.log1p(dataset_cat_df_dummies[['loss']].astype('float'))\n  return data_frame, data_outcome"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53},{"cell_type":"code","source":["#########################################################\n# Fill in missing categoricals\n#########################################################\n\n# df test runs for different models\ntestRDD = sc.textFile(finalproject_path + '/test.csv')\\\n              .filter(lambda x: x != testheaders)\\\n\nX_train_, X_train_loss_ = transformRDDtoDF(trainRDD)\nX_test_, X_test_loss_ = transformRDDtoDF(testRDD)\n\ntrain_cols = X_train_.columns\ntest_cols = X_test_.columns\n\n# filling in missing categoricals\ntrain_diff = list(set(train_cols) - set(test_cols))\ntest_diff = list(set(test_cols) - set(train_cols))\nprint(train_diff)\nprint(test_diff)\n\nfor col in train_diff:\n  X_test[col] = 0\n\nfor col in test_diff:\n  X_train_[col] = 0\n\ntrain_cols = X_train_.columns\ntest_cols = X_test_.columns\n\nprint(list(set(train_cols) - set(test_cols)))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1586861669172-0/PythonShell.py:21: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\nFor all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n  from py4j.java_collections import ListConverter\n[&#39;cat109_AK&#39;, &#39;cat109_BF&#39;, &#39;cat109_BM&#39;, &#39;cat113_BE&#39;, &#39;cat110_CB&#39;, &#39;cat110_EH&#39;, &#39;cat101_U&#39;, &#39;cat90_G&#39;, &#39;cat110_DV&#39;, &#39;cat110_BN&#39;, &#39;cat109_BT&#39;, &#39;cat109_BV&#39;, &#39;cat111_D&#39;, &#39;cat110_EI&#39;, &#39;cat110_AF&#39;, &#39;cat110_BD&#39;, &#39;cat109_AG&#39;, &#39;cat110_BK&#39;, &#39;cat109_BP&#39;, &#39;cat101_N&#39;, &#39;cat110_AN&#39;, &#39;cat102_H&#39;, &#39;cat114_X&#39;, &#39;cat109_BY&#39;, &#39;cat109_B&#39;, &#39;cat105_R&#39;, &#39;cat110_BI&#39;, &#39;cat113_AC&#39;, &#39;cat92_F&#39;, &#39;cat113_T&#39;, &#39;cat109_J&#39;, &#39;cat110_H&#39;, &#39;cat105_S&#39;, &#39;cat102_J&#39;, &#39;cat109_CJ&#39;, &#39;cat89_I&#39;]\n[&#39;cat89_F&#39;, &#39;cat92_G&#39;, &#39;cat111_L&#39;, &#39;cat110_CA&#39;, &#39;cat109_AD&#39;, &#39;cat106_Q&#39;, &#39;cat92_E&#39;, &#39;cat96_H&#39;, &#39;cat110_BH&#39;, &#39;cat99_U&#39;, &#39;cat103_M&#39;, &#39;cat113_AA&#39;, &#39;cat110_EN&#39;, &#39;cat113_R&#39;]\n/local_disk0/tmp/1586861669172-0/PythonShell.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n[&#39;cat109_AK&#39;, &#39;cat109_BF&#39;, &#39;cat109_BM&#39;, &#39;cat113_BE&#39;, &#39;cat110_CB&#39;, &#39;cat110_EH&#39;, &#39;cat101_U&#39;, &#39;cat90_G&#39;, &#39;cat110_DV&#39;, &#39;cat110_BN&#39;, &#39;cat109_BT&#39;, &#39;cat109_BV&#39;, &#39;cat111_D&#39;, &#39;cat110_EI&#39;, &#39;cat110_AF&#39;, &#39;cat110_BD&#39;, &#39;cat109_AG&#39;, &#39;cat110_BK&#39;, &#39;cat109_BP&#39;, &#39;cat101_N&#39;, &#39;cat110_AN&#39;, &#39;cat102_H&#39;, &#39;cat114_X&#39;, &#39;cat109_BY&#39;, &#39;cat109_B&#39;, &#39;cat105_R&#39;, &#39;cat110_BI&#39;, &#39;cat113_AC&#39;, &#39;cat92_F&#39;, &#39;cat113_T&#39;, &#39;cat109_J&#39;, &#39;cat110_H&#39;, &#39;cat105_S&#39;, &#39;cat102_J&#39;, &#39;cat109_CJ&#39;, &#39;cat89_I&#39;]\n</div>"]}}],"execution_count":54},{"cell_type":"markdown","source":["## Algorithm Exploration\n\nThe baseline algorithmic model used with the data was a **kitchen sink linear regression model**. Expertly named, a kitchen sink model is a type of regression that uses as many independent variables as possible in order to explain away any potential variances found on the dependent variable. It gives way to the phrase, ‘everything but the kitchen sink’, which basically means throw everything in and see what happens. One issue with this type of model is that it can use too many of the independent variables, which will lead to an overfit model, an issue explained in the previous section.\n\nFor a simple comparison to the baseline model, **Ridge and Lasso regressions** were run to see if the baseline model could be improved with regularization. While an OLS estimator, like the one performed for the baseline model, generally has low bias, the variance can be quite high, especially if there are many dimensions, as are present in the Allstate data. Enter regularization with Ridge and Lasso regressions. Ridge Regression works to regularize a model by setting predictor’s coefficients that are too far from zero to be a very small value, which all but eliminates them from the model; the model has decreased in complexity without actually removing any variables. In order to make this model most efficient, the lambda parameter, also called the regularization penalty coefficient, must be set in such a way that the bias and variance are balanced. Too high of a value causes the variance to decrease, but also leads to an increase in bias. Ridge Regression is also referred to as the L2 loss function. The other form of regularization is a Lasso Regression, or L1 loss function, and is somewhat similar to the Ridge in that it can add an adjustment to non-zero coefficients, penalizing the sums of absolute values. This results in many coefficients being completely zeroed out if the lambda parameter is set too high.\n\nAnother algorithm considered was **KNN (k-Nearest Neighbors)**, which holds on to the assumption that “birds of a feather flock together’; in other words, similar things tend to exist within close proximity to each other. kNN works by calculating distances between data points. In order to do this, data scientists often use the Euclidean distance calculation between sets of data points (square the difference for each axis, sum these, and then take the square root) to accomplish this. The “k” comes into play as the variable for which we’d want to assign our number of neighbors. This variable can be modified for each algorithm run so an optimal value (lowest error) is used. A step-by-step approach to this algorithm (useful for pipelining) is as follows; once a k-value is selected, the distance between each datapoint is calculated. As each value is calculated, it, as well as its index are added to an ordered collection, which is sorted ascendingly by index and distance, with the smallest distance at the top of the stack. The first K entries are picked from the stack, labeled, and if the algorithm was called for regression, the mean of the group is returned; for classification, the mode would be returned. A few caveats to this method are that as the value for K approaches one, the predictions become less accurate because there is a higher chance that the query node point will select a neighbor from a different group instead of ones also nearby that have similar features. The model is also known to become slow to run if the amount of independent variables or predictors from the input dataset are too substantial.\n\nA **Decision Tree algorithm** falls within the class of ‘supervised learning’ algorithms, meaning that it uses a training model made up of simple decision rules in order to predict the target variable. The algorithm learns in terms of root nodes, leaf nodes, and inner nodes that when laid out, look like the roots of a tree, hence the name. The tree maps observations related to an item to conclusions about the item’s target value. Decision Trees are used for two types of problems; those with categorical variables and those with continuous variables. The model is a popular choice for machine learning problems for many reasons including the following; it doesn’t require any pre-processing, it can be used as a dimensionality reduction approach, they work better than a linear model when there is a high dimensionality between independent and predictor variables, they are very robust in the presence of missing data, and they don’t need to normalize or standardize the data. When building a decision tree, the root node represents all of the data and is split into branches or sub-nodes, commonly referred to as decision or inner nodes. Decision nodes are then split into either another decision branch, or they end with a leaf node, which is the final segment, sometimes called the terminal node. This is where the classification occurs. The goal of each branch is to reach “purity” at the leaf node, which basically means that we want the mean and variance to be as close to, or at, zero as possible. The previous purity measurement calculation for classification is known as entropy. Other measurements for purity classifications include information gain, which is a statistical property that computes the entropy difference before a node split, and then the average entropy after the split, and Reduction in Variance, which uses the standard formula for variance to find the split points with the lowest variance to be used as the official split. Though decision trees are a popular choice, there can be drawbacks, like the common problem of overfitting, which can occur if there are no stopping points set, such as setting a rule that says once a node contains ten or less samples, do not split again. If stop points are not set, the decision tree could end up creating one leaf for every single observation in the data given. One method to combat this will be introduced below.\n\nBy using a **Random Forest**, which is part of the Decision Tree toolbox, the problem of overfitting can be solved. “Random Forest is an example of ensemble learning, in which we combine multiple machine learning algorithms to obtain better predictive performance.” (source: https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4) Random Forests use a technique called Bagging, which basically builds ensembles using many different, yet random samples from the dataset; predictions from each of these learned trees are aggregated and compared, with the best solution being chosen by means of averaging the results. One can think of this technique as “crowd wisdom”, where low correlation between models is key. The main difference between a regular decision tree and a random forest is that when a decision tree chooses to split, every possible feature is considered with the one producing the most separation between the left and right nodes being picked, whereas in a random forest, because the subset of data may only have a handful of features, the training and node splitting are done based on those accessed features. Like the name implies, in a forest, there are many different trees, which ultimately means that we will see more variation amongst the trees, which leads to lower correlation and more diversification. \n\nThe final subset of the Decision Tree model that was looked at is **Gradient Boosting Decision Trees**. Similar to Random Forest, this method uses an ensemble of decision trees in its prediction methods, with the biggest difference being that it also includes a parameter called the Learning Rate. Calculations are done by first computing the residuals of each sample and then building a tree with the goal of predicting those residual values. The final prediction takes the average output and adds the learning rate multiplied by the residual predicted by the decision tree. A new set of residuals is then computed using the actual value minus the predicted, and in turn, this new set of residuals is used for the leaves of the next decision tree. Once all of the trees have been created, a final prediction is made by taking the mean target and summing with each of the ensemble tree residuals multiplied by the learning rate. \n\nSources for above summaries: \n- Ridge/Lasso - https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n- KNN - https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n- Random Forest - https://towardsdatascience.com/random-forest-a-powerful-ensemble-learning-algorithm-2bf132ba639d\n- GB DT - https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"],"metadata":{}},{"cell_type":"markdown","source":["### Different Models + CV (sklearn)"],"metadata":{}},{"cell_type":"markdown","source":["#### Sklearn Implementation\n\nWe implemented our sklearn models by using a 90/10 training split with cross validation to reduce a specific test set biasing our metrics. \n\nWe also took into consideration the following implementation details:\n\n- **Caching** - We cached all of our models and results (cross validated results, predictions and MAE), including the performance counters for each model run. In this way, we're able to leverage the local storage cache as a way to reuse the programmatic work we did prior. In addition, this allows us to quickly run (or rerun) certain models to get comparative run results independent of day or cluster usage.\n\n- **IO / Memory** - We monitored the memory usage of sklearn models avidly. We know because sklearn primarily uses CPU and memory for computation, and some of our models are large, this metric would be important to monitor, especially for larger models like KNN. We used the `.info()` function of data frames to judge complexity. \n\n- **Modularity / Code Reuse** - We opted to reuse and standarize as much code as possible, so most of our work are functionalized. This promotes consistency, efficiency, reduces errors or typos and creates a more readable flow. Our models reuse the same training and metrics functions as well as the same dataset split.\n\n- **Time Complexity** - We used performance counters in our code to monitor how long training a model takes, and how long prediction takes for any model. We cache these results. Some models are quite large and doesn't work in databricks in our shared cluster. These include KNN and RandomForest* if we are to run the entire dataset.\n\n- **Sampling** - We sampled the training data for a smaller dataset to run the larger and more time consuming models. These include KNN and RandomForest. This way, we're able to get an idea of how a model performs with different parameters, and if it's feasible as potentially feasible model. The sampling is built into the function that creates the model as a parameter."],"metadata":{}},{"cell_type":"code","source":["#########################################################\n# Initialization code \n#########################################################\n\nfrom sklearn import model_selection\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# initialize train test split\nX_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(X_train_, X_train_loss_, test_size=0.08, random_state=1)\n\n# initialize model caching variables\ncacheModels = []\ncacheModelNames = []\ncacheCVResults = []\ncacheMAE = {}\ncacheMAE['train'] = []\ncacheMAE['test'] = []\ncachePerf = {}\ncachePerf['fit'] = []\ncachePerf['predict'] = []\ncachePerf['metric'] = []\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":58},{"cell_type":"code","source":["#########################################################\n# Sklearn Helper Functions \n#########################################################\n\ndef transform_mae(y_true, y_pred, **kwargs):\n  '''Transforms predicted values to normalized mae'''\n  return mean_absolute_error(np.expm1(y_pred), np.expm1(y_true))\n\ndef pickle_dump():\n  '''Use dumps to convert cached objects to a serialized string'''\n  pickle.dump( cacheModels, open( \"/dbfs/FileStore/cacheModels.p\", \"wb\" ) )\n  pickle.dump( cacheModelNames, open( \"/dbfs/FileStore/cacheModelNames.p\", \"wb\" ) )\n  pickle.dump( cacheCVResults, open( \"/dbfs/FileStore/cacheCVResults.p\", \"wb\" ) )\n  pickle.dump( cacheMAE, open( \"/dbfs/FileStore/cacheMAE.p\", \"wb\" ) )\n  pickle.dump( cachePerf, open( \"/dbfs/FileStore/cachePerf.p\", \"wb\" ) )\n\n  display(dbutils.fs.ls(\"dbfs:/FileStore\"))\n\ndef print_summary_raw():\n  '''Prints a text version of run results'''\n  for count, item in enumerate(cacheMAE['train']):\n    print(cacheModelNames[count] + \" Train - \" + str(item))\n    print(cacheModelNames[count] + \" Test - \" + str(cacheMAE['test'][count]))\n    print(cacheModelNames[count] + \" Time Fit:\" + str(round(cachePerf['fit'][count]/60, 1)) + \"min\")\n    print(cacheModelNames[count] + \" Time Predict:\" + str(round(cachePerf['predict'][count]/60, 1)) + \"min\")\n    print(cacheModelNames[count] + \" Time Metric:\" + str(round(cachePerf['metric'][count]/60, 1)) + \"min\")\n    \ndef plotSidebySideBarCharts(data1, data1_lbl, data2, data2_lbl, labels, title):\n  '''Plots side by side bar charts'''\n  x = np.arange(len(labels))  \n  width = 0.4  \n\n  fig, ax = plt.subplots()\n  plt.xticks(rotation=90)\n  rects2 = ax.bar(x + width/2, data1, width, label=data1_lbl)\n  rects1 = ax.bar(x - width/2, data2, width, label=data2_lbl)\n  ax.set_title(title)\n  ax.set_xticks(x)\n  ax.set_xticklabels(labels)\n  ax.legend(loc=\"lower left\")\n\n  fig.tight_layout()\n\n  display(plt.show())\n  \ndef plotBoxPlotCVCharts(title):\n  '''Plots box plot charts'''\n  fig = plt.figure()\n  fig.suptitle(title)\n  ax = fig.add_subplot(111)\n  plt.boxplot(cacheCVResults[2:])\n  ax.set_xticklabels(cacheModelNames[2:])\n  plt.xticks(rotation=90)\n  display(plt.show())\n  \ndef plotLineChart(data1, data1_label, data2, data2_label, x_num, xLabel, yLabel, title, axes, fig, subplot):\n  '''Plots side by side subplots for line charts'''\n  axes[subplot].plot(x_num, data1, label=data1_label)\n  axes[subplot].plot(x_num, data2, label=data2_label)\n  axes[subplot].set_title(title)\n  axes[subplot].set(xlabel=xLabel, ylabel=yLabel)\n  axes[subplot].legend(loc=\"upper left\")\n  plt.legend()\n  \ndef addModelToCache(model, name, fulldataset = True):\n  '''Creates models for a given dataset, can be full or partial dataset'''\n  smallSet = 300\n  tic = time.perf_counter()\n  if (fulldataset == True):\n    model.fit(X_train_subset, y_train_subset)\n  else:\n    model.fit(X_train_subset[0:smallSet], y_train_subset[0:smallSet])\n    \n  cachePerf['fit'].append(time.perf_counter()-tic)\n  kfold = model_selection.KFold(n_splits = 5, random_state = 1)\n  \n  cv_results = model_selection.cross_val_score(model, X_train_subset[0:smallSet], y_train_subset[0:smallSet], cv=kfold, scoring=\"neg_mean_absolute_error\")\t\n  if (fulldataset == True):\n    cv_results = model_selection.cross_val_score(model, X_train_subset, y_train_subset, cv=kfold, scoring=\"neg_mean_absolute_error\")\t\n\n  tic = time.perf_counter()\n  y_pred_train = model.predict(X_train_subset)\n  y_pred_test = model.predict(X_test_subset)\n  cachePerf['predict'].append(time.perf_counter()-tic)\n\n  cacheModelNames.append(name)\n  cacheModels.append((name, model)) \n  \n  cacheCVResults.append(cv_results) \n  \n  tic = time.perf_counter()\n  cacheMAE['train'].append(mean_absolute_error(np.expm1(y_train_subset), np.expm1(y_pred_train))) \n  cacheMAE['test'].append(mean_absolute_error(np.expm1(y_pred_test), np.expm1(y_test_subset)))\n  cachePerf['metric'].append(time.perf_counter()-tic)\n  \n  return\n\n# make the custom scorer for transforming MAE\ncustom_scorer = make_scorer(transform_mae, greater_is_better=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"code","source":["#########################################################\n# Sklearn Models and Functions \n#########################################################\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nimport pickle\nimport os.path\nimport warnings\nfrom tabulate import tabulate\nwarnings.filterwarnings('ignore')\n\n# verbose debugging\ndebug = False\n\n# we try to pick up cached results, if it doesn't exist, we rerun all models\nif (os.path.isfile(\"/dbfs/FileStore/cacheModels.p\")):\n  cacheModels = pickle.load( open( \"/dbfs/FileStore/cacheModels.p\", \"rb\" ))\n  cacheModelNames = pickle.load( open( \"/dbfs/FileStore/cacheModelNames.p\", \"rb\" ))\n  cacheCVResults = pickle.load( open( \"/dbfs/FileStore/cacheCVResults.p\", \"rb\" ))\n  cacheMAE = pickle.load( open( \"/dbfs/FileStore/cacheMAE.p\", \"rb\" ))\n  cachePerf = pickle.load( open( \"/dbfs/FileStore/cachePerf.p\", \"rb\" ))\n  if (debug == True):\n    print(\"Number of models loaded: \" + str(len(cacheModels)))\n    print(\"Number of cross validated results loaded: \" + str(len(cacheCVResults) * len(cacheCVResults[0])))\nelse:\n  addModelToCache(LinearRegression(normalize=False), \"LinRegression\")\n  addModelToCache(LinearRegression(normalize=True), \"LinRegression (norm)\")\n  addModelToCache(DecisionTreeRegressor(max_depth=5), \"DTRegressor (d-5)\")\n  addModelToCache(DecisionTreeRegressor(max_depth=7), \"DTRegressor (d-7)\")\n  addModelToCache(DecisionTreeRegressor(max_depth=9), \"DTRegressor (d-9)\")\n\n  addModelToCache(Ridge(alpha=1.0), \"RidgeRegression (a-1)\")\n  addModelToCache(Ridge(alpha=3.0), \"RidgeRegression (a-3)\")\n  addModelToCache(RidgeCV(alphas=[1, 1e3, 1e6], store_cv_values=True), \"RidgeCVRegression (1,e3,e6)\")\n\n  addModelToCache(Lasso(alpha=0.05, max_iter=1000), \"LassoRegression (a-0.05, i-1000)\")\n  addModelToCache(Lasso(alpha=0.4, max_iter=1000), \"LassoRegression (a-0.4, i-1000)\")\n\n  addModelToCache(KNeighborsRegressor(n_neighbors=2), \"KNNRegression (n-2) ss\", False)\n  addModelToCache(KNeighborsRegressor(n_neighbors=3), \"KNNRegression (n-3) ss\", False)\n\n  addModelToCache(RandomForestRegressor(criterion='mae', n_estimators=40, random_state=0), \"RandomForestRegressor (n-40) ss\", False)\n  addModelToCache(RandomForestRegressor(criterion='mae', n_estimators=60, random_state=0), \"RandomForestRegressor (n-60) ss\", False)\n\n  addModelToCache(XGBRegressor(n_estimators=500, random_state=0), \"XGBRegressor (n-500) ss\")\n  addModelToCache(XGBRegressor(n_estimators=800, random_state=0), \"XGBRegressor (n-800) ss\")\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[17:11:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[17:11:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n</div>"]}}],"execution_count":60},{"cell_type":"code","source":["# Tabular representation of training and test MAE\ntable_data = []\nfor count, item in enumerate(cacheMAE['train']):\n  table_data.append([cacheModelNames[count], cacheMAE['train'][count], cacheMAE['test'][count]])\nprint(tabulate(table_data, headers=['Sklearn Model', 'Train MAE', 'Test MAE'], tablefmt=\"github\"))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| Sklearn Model                    |   Train MAE |   Test MAE |\n----------------------------------|-------------|------------|\n LinRegression                    |     1241.8  |    1233.32 |\n LinRegression (norm)             |     1243.61 |    1234.44 |\n DTRegressor (d-5)                |     1380.98 |    1375.64 |\n DTRegressor (d-7)                |     1322.55 |    1321.04 |\n DTRegressor (d-9)                |     1271.86 |    1294.06 |\n RidgeRegression (a-1)            |     1242.23 |    1233.58 |\n RidgeRegression (a-3)            |     1242.69 |    1233.42 |\n RidgeCVRegression (1,e3,e6)      |     1242.23 |    1233.58 |\n LassoRegression (a-0.05, i-1000) |     1545.82 |    1531    |\n LassoRegression (a-0.4, i-1000)  |     1810.82 |    1798.63 |\n KNNRegression (n-2) ss           |     1729.64 |    1712.61 |\n KNNRegression (n-3) ss           |     1663.01 |    1651.8  |\n RandomForestRegressor (n-40) ss  |     1429.59 |    1416.86 |\n RandomForestRegressor (n-60) ss  |     1423.44 |    1409.58 |\n XGBRegressor (n-500) ss          |     1152.1  |    1156.66 |\n XGBRegressor (n-800) ss          |     1139.21 |    1151.32 |\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["plotSidebySideBarCharts(cacheMAE['train'], \"Train\", cacheMAE['test'], \"Test\", cacheModelNames, 'Train and Test MAE across Sklearn Models')\n"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["plotBoxPlotCVCharts(\"Sklearn regression k-fold cross-validation between models\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["##### Train and test MAE across sklearn models\n- XGBRegressor has the best results for both train and test set. cross validation also shows smaller variation. Low bias and low variance.\n- Kitchen sink baseline linear regression model and ridge regression come second which is a bit surprising because we expect linear regression would overfit. Cross validation shows small variation for both models as well. Low bias. Ridge should have lower variance as it's regularized.\n- Decision tree comes third in terms of goodness of the fit. Higher depth yields better results. Cross validation shows small variation.\n- KNN and random forest regressor have large MAE and large variation during cross validation - this is likely due to the sampled data set."],"metadata":{}},{"cell_type":"code","source":["# Tabular representation of fit and predict times\ntable_data = []\nfor count, item in enumerate(cacheMAE['train']):\n  table_data.append([cacheModelNames[count], cachePerf['fit'][count], cachePerf['predict'][count]])\nprint(tabulate(table_data, headers=['Sklearn Model', 'Fit Time (s)', 'Predict Time (s)'], tablefmt=\"github\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| Sklearn Model                    |   Fit Time (s) |   Predict Time (s) |\n----------------------------------|----------------|--------------------|\n LinRegression                    |     5.81621    |           0.878148 |\n LinRegression (norm)             |     6.25225    |           0.878511 |\n DTRegressor (d-5)                |     6.80338    |           1.25978  |\n DTRegressor (d-7)                |     9.45756    |           1.27125  |\n DTRegressor (d-9)                |    12.0693     |           1.25893  |\n RidgeRegression (a-1)            |     2.49558    |           0.879778 |\n RidgeRegression (a-3)            |     2.59677    |           0.879714 |\n RidgeCVRegression (1,e3,e6)      |    16.1217     |           0.885995 |\n LassoRegression (a-0.05, i-1000) |     4.76562    |           0.866206 |\n LassoRegression (a-0.4, i-1000)  |     2.2528     |           0.943981 |\n KNNRegression (n-2) ss           |     0.00559503 |          84.2116   |\n KNNRegression (n-3) ss           |     0.00552916 |          83.8078   |\n RandomForestRegressor (n-40) ss  |     5.03507    |           2.65907  |\n RandomForestRegressor (n-60) ss  |     7.30535    |           3.36258  |\n XGBRegressor (n-500) ss          |  1298.77       |           8.17456  |\n XGBRegressor (n-800) ss          |  2104.53       |           9.7408   |\n</div>"]}}],"execution_count":65},{"cell_type":"code","source":["plotSidebySideBarCharts(cachePerf['fit'], \"Fit\", cachePerf['predict'], \"Predict\", cacheModelNames, 'Fit and Predict Time across Sklearn Models')"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["##### Fit and predict times across sklearn models\n- XGBregressor takes the longest to train the model but takes much less time to predict. It should scale up well.\n- KNN doesn't work (times out) on sklearn with the full dataset. Even on a smaller dataset, it takes a long time to predict because it needs to evaluate train set data for every prediction. It does not scale up well in sklearn.\n- Decision tree, ridge/lasso, linear, and random forest models all take similar time to train and predict."],"metadata":{}},{"cell_type":"markdown","source":["### Add on PCA in sklearn pipeline"],"metadata":{}},{"cell_type":"markdown","source":["PCA + linear regression (sklearn)"],"metadata":{}},{"cell_type":"code","source":["#normalize target variable using MinMaxScaler\nscaled_clf = make_pipeline(PCA(n_components=200), LinearRegression())\nscaled_clf = scaled_clf.fit(X_train, y_train_scaled)\n\npred_test = np.expm1(scaler.inverse_transform(scaled_clf.predict(X_test)))\npred_train = np.expm1(scaler.inverse_transform(scaled_clf.predict(X_train)))\nprint(mean_absolute_error(pred_train, np.expm1(y_train)))\nprint(mean_absolute_error(pred_test, np.expm1(y_test)))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1252.8789767913954\n1271.692541210633\n</div>"]}}],"execution_count":70},{"cell_type":"markdown","source":["PCA + Lasso (sklearn)"],"metadata":{}},{"cell_type":"code","source":["#try a different transformation\npt = PowerTransformer()\ny_train_trans = pt.fit_transform(y_train)\ny_test_trans = pt.transform(y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"code","source":["clf = make_pipeline(PCA(n_components=200), \n                    GridSearchCV(Lasso(),\n                                 param_grid={'alpha': [0.01,1,5,10]},\n                                 cv=5,\n                                 refit=True))\n\nclf.fit(X_train, y_train_trans)\ny_pred_test = np.expm1(pt.inverse_transform(pd.DataFrame(clf.predict(X_test))))\ny_pred_train =np.expm1(pt.inverse_transform(pd.DataFrame(clf.predict(X_train))))\nprint(round(mean_absolute_error (np.expm1(y_train), y_pred_train), 5))\nprint(round(mean_absolute_error (np.expm1(y_test), y_pred_test), 5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1311.03831\n1334.03043\n</div>"]}}],"execution_count":73},{"cell_type":"markdown","source":["PCA + Decision Tree (sklearn)"],"metadata":{}},{"cell_type":"code","source":["# prepare the model with input scaling\ntree = make_pipeline(PCA(n_components=200), \n                    GridSearchCV(DecisionTreeRegressor(),\n                                 param_grid={'max_depth': [5,10,15,20]},\n                                 cv=10,\n                                 refit=True))\n\ntree.fit(X_train, y_train_trans)\ny_pred_test_tree = np.expm1(pt.inverse_transform(pd.DataFrame(tree.predict(X_test))))\ny_pred_train_tree =np.expm1(pt.inverse_transform(pd.DataFrame(tree.predict(X_train))))\nprint(mean_absolute_error (np.expm1(y_train), y_pred_train_tree), 5)\nprint(mean_absolute_error (np.expm1(y_test), y_pred_test_tree), 5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1258.6682880522237 5\n1367.9410565118071 5\n</div>"]}}],"execution_count":75},{"cell_type":"markdown","source":["### Different Models Comparison (pyspark.ml)"],"metadata":{}},{"cell_type":"markdown","source":["Pyspark pipeline\n\nWe had some scale issues with KNN and RandomForest in sklearn. To draw a comparison between sklearn and pyspark, we ran the same models in pyspark ML.  \nUsing similar transformations, we one-hot encoded using th pyspark OneHotEncoderEstimator and VectorAssembler to create the pipeline. \n\nUnlike sklearn where each step was executed as it was coded, the pyspark implementation was lazy, meaning it only encoded and assembled features once we asked for a model fit."],"metadata":{}},{"cell_type":"code","source":["#########################################################\n# Spark Setup \n#########################################################\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom operator import add\n\nimport pickle\nimport os.path\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncacheSparkModels = []\ncacheSparkModelNames = []\ncacheSparkCVResults = []\ncacheSparkMAE = {}\ncacheSparkMAE['train'] = []\ncacheSparkMAE['test'] = []\ncacheSparkPerf = {}\ncacheSparkPerf['fit'] = []\ncacheSparkPerf['predict'] = []\ncacheSparkPerf['metric'] = []\n\ndef readEncodeTransformData():\n  '''Reads and transforms data for spark modeling from csv'''\n  \n  # Load training data\n  # infer data type (cat, double, etc)\n  training_spark_raw = spark.read.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(finalproject_path + '/train.csv')\n\n  categorical_columns = [\"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\",\"cat7\",\"cat8\",\"cat9\",\"cat10\",\"cat11\",\"cat12\",\"cat13\",\"cat14\",\"cat15\",\"cat16\",\"cat17\",\"cat18\",\"cat19\",\"cat20\",\"cat21\",\"cat22\",\"cat23\",\"cat24\",\"cat25\",\"cat26\",\"cat27\",\"cat28\",\"cat29\",\"cat30\",\"cat31\",\"cat32\",\"cat33\",\"cat34\",\"cat35\",\"cat36\",\"cat37\",\"cat38\",\"cat39\",\"cat40\",\"cat41\",\"cat42\",\"cat43\",\"cat44\",\"cat45\",\"cat46\",\"cat47\",\"cat48\",\"cat49\",\"cat50\",\"cat51\",\"cat52\",\"cat53\",\"cat54\",\"cat55\",\"cat56\",\"cat57\",\"cat58\",\"cat59\",\"cat60\",\"cat61\",\"cat62\",\"cat63\",\"cat64\",\"cat65\",\"cat66\",\"cat67\",\"cat68\",\"cat69\",\"cat70\",\"cat71\",\"cat72\",\"cat73\",\"cat74\",\"cat75\",\"cat76\",\"cat77\",\"cat78\",\"cat79\",\"cat80\",\"cat81\",\"cat82\",\"cat83\",\"cat84\",\"cat85\",\"cat86\",\"cat87\",\"cat88\",\"cat89\",\"cat90\",\"cat91\",\"cat92\",\"cat93\",\"cat94\",\"cat95\",\"cat96\",\"cat97\",\"cat98\",\"cat99\",\"cat100\",\"cat101\",\"cat102\",\"cat103\",\"cat104\",\"cat105\",\"cat106\",\"cat107\",\"cat108\",\"cat109\",\"cat110\",\"cat111\",\"cat112\",\"cat113\",\"cat114\",\"cat115\",\"cat116\"]\n\n  # index categorical columns\n  strindexers = [\n      StringIndexer(inputCol = col_, outputCol = \"{0}_indexed\".format(col_))\n      for col_ in categorical_columns\n  ]\n\n  # one-hot encode categorical columns\n  encoder = OneHotEncoderEstimator(\n      inputCols = [indexer.getOutputCol() for indexer in strindexers],\n      outputCols = [\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in strindexers]\n  )\n\n  # get all features (encoded and continuous)\n  all_features = encoder.getOutputCols() + [\"cont1\",\"cont2\",\"cont3\",\"cont4\",\"cont5\",\"cont6\",\"cont7\",\"cont8\",\"cont9\",\"cont10\",\"cont11\",\"cont12\",\"cont13\",\"cont14\"]\n\n  # assemble into a pipeline\n  assembler = VectorAssembler(\n      inputCols = all_features,\n      outputCol = \"features\"\n  )\n\n  # fit and actually do the work\n  pipeline = Pipeline(stages=strindexers + [encoder, assembler])\n  training_transformed = pipeline.fit(training_spark_raw).transform(training_spark_raw)\n  \n  return training_transformed\n\n#########################################################\n# Spark Initialization and Partition Code\n#########################################################\n\ntraining_transformed = readEncodeTransformData()\ntraining_transformed.take(1)\n\n# spark split into training and test sets \nsplits = training_transformed.randomSplit([0.9, 0.1])\ntraining_transformed_df = splits[0]\ntest_transformed_df = splits[1]\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":78},{"cell_type":"code","source":["#########################################################\n# Spark Helper Functions\n#########################################################\n\ndef pickle_dump_spark():\n  '''Use dumps to convert objects to a serialized string. does not work for spark models '''\n  # pickle.dump( cacheModels, open( \"/dbfs/FileStore/cacheModels.p\", \"wb\" ) )\n  pickle.dump( cacheSparkModelNames, open( \"/dbfs/FileStore/cacheModelNames.p\", \"wb\" ) )\n  pickle.dump( cacheSparkMAE, open( \"/dbfs/FileStore/cacheMAE.p\", \"wb\" ) )\n  pickle.dump( cacheSparkPerf, open( \"/dbfs/FileStore/cacheMAE.p\", \"wb\" ) )\n\n  display(dbutils.fs.ls(\"dbfs:/FileStore\"))\n\ndef print_summary_raw_spark():\n  '''Prints raw summary for spark models'''\n  for count, item in enumerate(cacheSparkMAE['train']):\n    print(cacheSparkModelNames[count] + \" Train - \" + str(item))\n    print(cacheSparkModelNames[count] + \" Test - \" + str(cacheSparkMAE['test'][count]))\n    print(cacheSparkModelNames[count] + \" Time Fit:\" + str(round(cacheSparkPerf['fit'][count]/60, 1)) + \"min\")\n    print(cacheSparkModelNames[count] + \" Time Predict:\" + str(round(cacheSparkPerf['predict'][count]/60, 1)) + \"min\")\n    print(cacheSparkModelNames[count] + \" Time Metric:\" + str(round(cacheSparkPerf['metric'][count]/60, 1)) + \"min\")\n\n\ndef addSparkModelToCache(model, name, fulldataset = True):\n  '''Runs spark models for a transformed dataset, saves results'''\n  # initialize timers\n  tic = time.perf_counter()\n  lr_model = model.fit(training_transformed_df)\n  cacheSparkPerf['fit'].append(time.perf_counter()-tic)\n      \n  # transform and predict training and test data\n  tic = time.perf_counter()\n  y_train_predictions = lr_model.transform(training_transformed_df)\n  y_test_predictions = lr_model.transform(test_transformed_df)\n  cacheSparkPerf['predict'].append(time.perf_counter()-tic)\n\n  # initialize regression evaluator\n  tic = time.perf_counter()\n  dt_evaluator = RegressionEvaluator(\n      labelCol=\"loss\", predictionCol=\"prediction\", metricName=\"mae\")\n  \n  mae_train = dt_evaluator.evaluate(y_train_predictions)\n  mae_test = dt_evaluator.evaluate(y_test_predictions)\n  # cache all results\n  cacheSparkPerf['metric'].append(time.perf_counter()-tic)\n\n  cacheSparkModelNames.append(name)\n  cacheSparkModels.append((name, lr_model)) \n  \n  cacheSparkMAE['train'].append(mae_train) \n  cacheSparkMAE['test'].append(mae_test)\n                          \n  print(name + \" Train:\" + str(mae_train))\n  print(name + \" Test:\" + str(mae_test))\n  \n  return\n  \ndef addPCAResultsFromAnotherNoteBook():\n  '''adds PCA results to cached results'''\n  cacheSparkMAE['train'].append(1418.92)\n  cacheSparkMAE['test'].append(1432.34)\n  cacheSparkPerf['fit'].append(1662.6)\n  cacheSparkPerf['metric'].append(367.2)\n  cacheSparkPerf['predict'].append(6.3)\n  cacheSparkModelNames.append(\"Spk DecisionTreeRegressor + PCA\")\n  cacheResultsModels()\n  \ndef cacheResultsModels():\n  '''pickles cached results'''\n  pickle.dump( cacheSparkModelNames, open( \"/dbfs/FileStore/cacheSparkModelNames.p\", \"wb\" ) )\n  pickle.dump( cacheSparkMAE, open( \"/dbfs/FileStore/cacheSparkMAE.p\", \"wb\" ) )\n  pickle.dump( cacheSparkPerf, open( \"/dbfs/FileStore/cacheSparkPerf.p\", \"wb\" ) )\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":79},{"cell_type":"code","source":["#########################################################\n# Spark Models and Cache\n#########################################################\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.regression import GBTRegressor\n\nif (os.path.isfile(\"/dbfs/FileStore/cacheSparkPerf.p\")):\n  cacheSparkModelNames = pickle.load( open( \"/dbfs/FileStore/cacheSparkModelNames.p\", \"rb\" ))\n  #cacheSparkCVResults = pickle.load( open( \"/dbfs/FileStore/cacheSparkCVResults.p\", \"rb\" ))\n  cacheSparkMAE = pickle.load( open( \"/dbfs/FileStore/cacheSparkMAE.p\", \"rb\" ))\n  cacheSparkPerf = pickle.load( open( \"/dbfs/FileStore/cacheSparkPerf.p\", \"rb\" ))\n  print(\"Number of models loaded: \" + str(len(cacheSparkModelNames)))\nelse:\n  addSparkModelToCache(LinearRegression(featuresCol = 'features', labelCol='loss', regParam=0.0), \"Spk LinRegression\")\n\n  addSparkModelToCache(LinearRegression(featuresCol = 'features', labelCol='loss', regParam=1.0, elasticNetParam = 0), \"Spk RidgeRegression (a-1)\")\n  addSparkModelToCache(LinearRegression(featuresCol = 'features', labelCol='loss', regParam=3.0, elasticNetParam = 0), \"Spk RidgeRegression (a-3)\")\n  addSparkModelToCache(LinearRegression(featuresCol = 'features', labelCol='loss', regParam=0.05, elasticNetParam = 1), \"Spk LassoRegression  (a-0.05)\")\n  addSparkModelToCache(LinearRegression(featuresCol = 'features', labelCol='loss', regParam=0.4, elasticNetParam = 1), \"Spk LassoRegression  (a-0.4)\")\n  addSparkModelToCache(DecisionTreeRegressor(featuresCol = 'features', labelCol='loss', maxDepth=5), \"Spk DecisionTreeRegressor  (d-5)\")\n\n  addSparkModelToCache(DecisionTreeRegressor(featuresCol = 'features', labelCol='loss', maxDepth=7), \"Spk DecisionTreeRegressor (d-7)\")\n  addSparkModelToCache(DecisionTreeRegressor(featuresCol = 'features', labelCol='loss', maxDepth=9), \"Spk DecisionTreeRegressor (d-9)\")\n  addSparkModelToCache(RandomForestRegressor(featuresCol = 'features', labelCol='loss', numTrees=40), \"Spk RandomForestRegressor (n-40)\")\n  addSparkModelToCache(RandomForestRegressor(featuresCol = 'features', labelCol='loss', numTrees=60), \"Spk RandomForestRegressor (n-60)\")\n  addSparkModelToCache(GBTRegressor(featuresCol = 'features', labelCol='loss', maxIter=10), \"Spk GBTRegressor\")\n  cacheResultsModels()\n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of models loaded: 13\n</div>"]}}],"execution_count":80},{"cell_type":"code","source":["table_data = []\nfor count, item in enumerate(cacheSparkMAE['train']):\n  table_data.append([cacheSparkModelNames[count], cacheSparkMAE['train'][count], cacheSparkMAE['test'][count]])\nprint(tabulate(table_data, headers=['Spark Model', 'Train MAE', 'Test MAE'], tablefmt=\"github\"))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| Spark Model                      |   Train MAE |   Test MAE |\n----------------------------------|-------------|------------|\n Spk LinRegression                |     1287.44 |    1318.82 |\n Spk RidgeRegression (a-1)        |     1287.12 |    1319.08 |\n Spk RidgeRegression (a-3)        |     1287.18 |    1318.62 |\n Spk LassoRegression  (a-0.05)    |     1287.68 |    1318.78 |\n Spk LassoRegression  (a-0.4)     |     1287.04 |    1317.73 |\n Spk DecisionTreeRegressor  (d-5) |     1438.49 |    1452.49 |\n Spk DecisionTreeRegressor (d-7)  |     1387.06 |    1408.81 |\n Spk DecisionTreeRegressor (d-9)  |     1335.4  |    1379.78 |\n Spk RandomForestRegressor (n-40) |     1423.62 |    1434.81 |\n Spk RandomForestRegressor (n-60) |     1424.2  |    1433.38 |\n Spk GBTRegressor                 |     1328.99 |    1354.42 |\n Spk GBTRegressor (n-12)          |     1315.19 |    1342.4  |\n Spk DecisionTreeRegressor + PCA  |     1418.92 |    1432.34 |\n</div>"]}}],"execution_count":81},{"cell_type":"code","source":["plotSidebySideBarCharts(cacheSparkMAE['train'], \"Train\", cacheSparkMAE['test'], \"Test\", cacheSparkModelNames, 'Train and Test MAE across pySpark Models')"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["##### Train and test MAE across spark models\n\nFrom the results, the MAE for test and train were not drastically different from sklearn, which is expected since they are the same models. Overall, most of the test MAE was a little higher than training MAE.The MAE was similar between linear, ridge and lasso regression. The interesting thing here is that as depth increased for the decision tree parameter, the model got incrementally better, which suggests there's room for tuning.\n\n\nSome other observations:\n- Lasso regression has the smallest mae for test set, but the advantage is very small. Ridge and baseline yield similar results.\n- Gradient boost tree comes in third \n- Other tree models did not yield great results, but decision tree did better as the depth increased for both training and test.\n- PCA also does not seem to have large effect, but expectedly increased MAE"],"metadata":{}},{"cell_type":"code","source":["table_data = []\nfor count, item in enumerate(cacheSparkMAE['train']):\n  table_data.append([cacheSparkModelNames[count], cacheSparkPerf['fit'][count], cacheSparkPerf['metric'][count]+cacheSparkPerf['predict'][count]])\nprint(tabulate(table_data, headers=['Spark Model', 'Fit Time (s)', 'Predict Time (s)'], tablefmt=\"github\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| Spark Model                      |   Fit Time (s) |   Predict Time (s) |\n----------------------------------|----------------|--------------------|\n Spk LinRegression                |       1076.26  |            588.515 |\n Spk RidgeRegression (a-1)        |       1018.6   |            543.341 |\n Spk RidgeRegression (a-3)        |        889.082 |            366.84  |\n Spk LassoRegression  (a-0.05)    |        730.39  |            317.494 |\n Spk LassoRegression  (a-0.4)     |       1235.99  |            605.855 |\n Spk DecisionTreeRegressor  (d-5) |        864.239 |            338.664 |\n Spk DecisionTreeRegressor (d-7)  |        833.758 |            681.93  |\n Spk DecisionTreeRegressor (d-9)  |        869.617 |            319.804 |\n Spk RandomForestRegressor (n-40) |        948.974 |            618.219 |\n Spk RandomForestRegressor (n-60) |        924.32  |            405.059 |\n Spk GBTRegressor                 |        820.988 |            311.832 |\n Spk GBTRegressor (n-12)          |       1699.1   |            642.187 |\n Spk DecisionTreeRegressor + PCA  |       1662.6   |            373.5   |\n</div>"]}}],"execution_count":84},{"cell_type":"code","source":["cacheSparkPerf['predict_metric'] = list( map(add, cacheSparkPerf['predict'], cacheSparkPerf['metric']) )\n\nplotSidebySideBarCharts(cacheSparkPerf['fit'], \"Fit\", cacheSparkPerf['predict_metric'], \"Predict\", cacheSparkModelNames, 'Fit and Predict Time across Spark Models')"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["##### Fit and predict times across spark models\n- This chart show the fit and predict times for our pyspark models. \n- The first thing is that no matter what the model is, most of the training (fit) times are quite similar, which was not true for sklearn (some models were on the order of seconds, whereas others did not work in sklearn with the full dataset on databricks). \n- PCA + decision tree takes the longest to train but not the longest to predict. It has to go through training twice, once for PCA and another time for decision tree, so understandably, it takes longer to complete."],"metadata":{}},{"cell_type":"code","source":["side_by_side_labels = ['LinRegression', 'RidgeRegression (a-1)', 'RidgeRegression (a-3)', 'LassoRegression', 'DecisionTreeRegressor (d-5)', 'DecisionTreeRegressor (d-7)', 'DecisionTreeRegressor (d-9)']\nside_by_side_values_sk_fit = [cachePerf['fit'][0], cachePerf['fit'][5], cachePerf['fit'][6], cachePerf['fit'][8], cachePerf['fit'][2], cachePerf['fit'][3], cachePerf['fit'][4]]\nside_by_side_values_spk_fit = [cacheSparkPerf['fit'][0], cacheSparkPerf['fit'][1], cacheSparkPerf['fit'][2], cacheSparkPerf['fit'][3], cacheSparkPerf['fit'][5], cacheSparkPerf['fit'][6], cacheSparkPerf['fit'][7]]\n\nside_by_side_values_sk_predict = [cachePerf['predict'][0], cachePerf['predict'][5], cachePerf['predict'][6], cachePerf['predict'][8], cachePerf['predict'][2], cachePerf['predict'][3], cachePerf['fit'][4]]\nside_by_side_values_spk_predict = [cacheSparkPerf['predict_metric'][0], cacheSparkPerf['predict_metric'][1], cacheSparkPerf['predict_metric'][2], cacheSparkPerf['predict_metric'][3], cacheSparkPerf['predict_metric'][5], cacheSparkPerf['predict_metric'][6], cacheSparkPerf['predict_metric'][7]]\n\nplotSidebySideBarCharts(side_by_side_values_sk_fit, \"Sklearn Fit Time\", side_by_side_values_spk_fit, \"Spark Fit Time\", side_by_side_labels, 'Sklearn and Spark Fit Time Comparison')\nplotSidebySideBarCharts(side_by_side_values_sk_predict, \"Sklearn Predict Time\", side_by_side_values_spk_predict, \"Spark Predict Time\", side_by_side_labels, 'Sklearn and Spark Predict Time Comparison')"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["##### Fit and predict times between spark and sklearn models\n- Spark models takes much longer to train and predict across the board for linear, ridge, lasso and decision tree regressors. The dataset is very small, so spark does not have advantage as it has larger overhead and speeds up slow."],"metadata":{}},{"cell_type":"markdown","source":["####On Random Forests and Nearest Neighbors"],"metadata":{}},{"cell_type":"code","source":["side_by_side_labels = ['RandomForestRegressor (n-40)', 'RandomForestRegressor (n-60)']\nside_by_side_labels_num = [40, 60]\nside_by_side_values_sk_fit = [cachePerf['fit'][12], cachePerf['fit'][13]]\nside_by_side_values_spk_fit = [cacheSparkPerf['fit'][8], cacheSparkPerf['fit'][9]]\n\nside_by_side_values_sk_predict = [cachePerf['predict'][12], cachePerf['predict'][13]]\nside_by_side_values_spk_predict = [cacheSparkPerf['predict_metric'][8], cacheSparkPerf['predict_metric'][8]]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\nplt.tight_layout()\nplotLineChart(side_by_side_values_sk_fit, \"Sklearn Fit Time\", side_by_side_values_sk_predict, \"Sklearn Predict Time\", side_by_side_labels_num, 'Number of Estimators', 'Execution Time', \"RandomForest Sklearn Fit + Predict Time vs. Number of Estimators\", axes, fig, 0)\n\nplotLineChart(side_by_side_values_spk_fit, \"Spark Fit Time\", side_by_side_values_spk_predict, \"Spark Predict Time\", side_by_side_labels_num, 'Number of Estimators', 'Execution Time', \"RandomForest Spark Fit + Predict Time vs. Number of Estimators\", axes, fig, 1)\ndisplay(fig.tight_layout())"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["##### Random forests demonstrate spark scalability\n\nFor the random forest runs, we were not able to run the full dataset on sklearn, but were able to run it in spark. Indeed, even on the smaller data set that random forest was run on, we saw an incremental increase in execution time as we scaled up the number of estimators. **Spark, on the other hand, did not have an incremental increase in execution time, likely due to the parallelization in the background for tree generation.**"],"metadata":{}},{"cell_type":"markdown","source":["### Preferred Algorithm\n\n#### Decision Trees"],"metadata":{}},{"cell_type":"markdown","source":["Given that Allstate claims’ dataset has 116 categorical variables along with 14 continuous variables, our consensus approach was to start with decision tree model as our preferred algorithm. \n\n**Why**\n\nIn addition to the point that both mutliple discrete variables and continuous variables can be easily accommodated, decision trees are able to capture non-linear interaction between the features and the label (“loss”). \nIt is also easy to understand with human perspective by graphically representing the process of decision-making. For a model that would likely be transformed into an algorithm or service for decision makers at Allstate to predict claims loss, it's important for them to know how the model operates, and update individual portions of the model as needed. \nDecisions trees can offer that modularity without rerunning the entire model, as well as easily accomodate new inputs, which would be likely for future iterations of this dataset."],"metadata":{}},{"cell_type":"markdown","source":["Here’s a small example of 6 data points. Although the variables in the actual dataset are all masked, we think these fields are likely included. \n\n\n|id\t|Age group\t|City/Rural\t|Income\t|Claims loss|\n|---|---|---|---|---|---|\n|X1 |’A(20-40)’ |‘C’ |0.75 |2213.18|\n|X2 |‘B(40-60)’ |‘R’ |0.25 |1283.6|\n|X3 |‘A(20-40)’ |‘R’ |0.25 |1132.22| \n|X4 |‘B(40-60)’ |‘C’ |0.60 |5142.87|\n|X5 |‘B(40-60)’ |‘C’ |0.52 |2142.87|\n|X6 |‘C(60-80)’ |‘C’ |0.40 |3005.09|"],"metadata":{}},{"cell_type":"markdown","source":["As in our feature engineering, discrete fields are transformed via the method of one-hot encoding to values between (0,1). Then PCA for dimension reduction is applied since the size determines the complexity of the tree and has to be neither too simple or too big leading to overfitting.  \nNow the transformed data points will look like this. \n\n|id\t|Age group |City/Rural\t|Income\t|Claims loss|\n|---|---|---|---|---|---|---|\n|X1 |0.25|1 |0.75 |2213.18|\n|X2 |0.5 |0 |0.25 |1283.6|\n|X3 |0.25|0 |0.25 |1132.22| \n|X4 |0.5 |1 |0.60 |5142.87|\n|X5 |0.5 |1 |0.52 |2142.87|\n|X6 |0.75 |1 |0.40 |3005.09|\n\n* assuming X2 and X3 are grouped into one category after PCA"],"metadata":{}},{"cell_type":"markdown","source":["This gives us the decision tree flow chart as below. \n\n![Decision Tree Flow Chart](https://s3-us-west-2.amazonaws.com/sophiaxcui.com/images/image1.png)"],"metadata":{}},{"cell_type":"markdown","source":["We define the loss function L(y, yˆ) as Mean Absolute Error, which is the variance in the target “loss” terms. MAE is appropriate for Allstate insurance claims due to the fact that all the individual “loss” differences are weighted equally in the average. (Hence, the models with the lowest MAEs will be ideal from the bias-variance trade-off.)\nThe calculation of MAE is,\n$$\nMean Absolute Error (Variance) = \\frac{1}{n}\\sum\\_{i=1}^{n}\\left| y\\_i\\ - \\mu\\right|\n$$\nTherefore the small example data points yields MAE of 525.23"],"metadata":{}},{"cell_type":"markdown","source":["|id\t|Age group\t|City/Rural\t|Income\t|Claims loss(A)|| Prediction(B) | MAE_i (A-B)|\n|---|---|---|---|---|---|---||---|---|\n|X1 |0.25|1 |0.75 |2213.18||2213.18| 0 |\n|X2 |0.5 |0 |0.25 |1283.6||1207.91| 75.69 |\n|X3 |0.25|0 |0.25 |1132.22|| 1207.91| 75.69 |\n|X4 |0.5 |1 |0.60 |5142.87||3642.87 | 1500 |\n|X5 |0.5 |1 |0.52 |2142.87||3642.87| 1500 |\n|X6 |0.75 |1 |0.40 |3005.09||3005.09| 0 |\n$$\nMAE of decision tree = \\frac{3151.38}{6} = 525.23\n$$"],"metadata":{}},{"cell_type":"markdown","source":["### Conclusion\n\nThe Allstate dataset is unique because it's just small enough to run sklearn but also large enough where pyspark could make a difference in execution complexity.\nHence, we explored the Allstate dataset through two different methodologies, running a dozen models through sklearn and those same models through pyspark ml libraries.\n\n**Some salient insights include:**\n- Both pipelines yielded similar evaluation metrics (ours was MAE)\n- XGBoostRegressor had the lowest MAE, consistently, and by far across all models, and would be our choice if the only salient point of consideration was MAE (Kaggle)\n- Due to the large number correlated of correlated inputs, PCA helped reduce the number of inputs, but it did not improve MAE\n- For less computationally expensive models (linear regression, ridge, lasso), sklearn is indeed much faster by magnitudes.  \n- For more computationally heavy models (KNN, RandomForest), spark scales significantly better, especially with respect to parameter tuning \n- For our preferred model, Spark Decision Tree Regression, we saw an increase in computational time for sklearn as the depth of the tree increased, but for pyspark, this was not observed.\n\n**Real world implications and future extensions:**\nThe Allstate dataset has many anonymized inputs, and due to the nature of claims being modeled by many different factors, future versions of this dataset can include magnitudes more inputs. We can imagine a IoT world where a very large amount of device or personal data can be fed in to create a more complete picture for any given claim. In this sense, a blackbox solution is not always best, especially with the sensitive nature of judging a claim by its cover (e.g. demographic data like race, gender, income, etc.). In addition, introducing implicit social biases in a black box model would be very hard to detect or fix until its too late. See [Amazon scraps internal AI recruiting tool that was biased against women](https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report). \n\nOur preferred model, pyspark decision tree regression, does ok in the pack of regressors in terms of MAE (our metric for model accuracy). However, it's very easy to understand and interpret, as well as being able to modify to accomodate new inputs. Mostly, the inherent structure of a decision tree aligns well with decisions made by humans, and can be manually adjusted easily should the model become a source of contention for bias or lawsuits. Moreover, it's scalable and can be modularized for fine tuning decisions made by a subset of data, or additional data."],"metadata":{}}],"metadata":{"name":"Team10_allstate_data_EDA","notebookId":1531046705540644},"nbformat":4,"nbformat_minor":0}
